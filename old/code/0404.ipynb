{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os, copy, argparse, configparser\n",
    "import sys, datetime, csv, random\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.utils import save_image\n",
    "import network_models as models\n",
    "import function_losses as losses\n",
    "import pytorch_model_summary as mosum\n",
    "from PIL import Image\n",
    "\n",
    "# ======================================================================\n",
    "# take options \n",
    "# ======================================================================\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dir_work\", type=str, default='/nas/users/minhyeok/energy_based_model')\n",
    "parser.add_argument(\"--device_cuda\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--model_conv\", type=str, default='conv_double_resnet')\n",
    "parser.add_argument(\"--model_activation\", type=str, default='leakyrelu')\n",
    "parser.add_argument(\"--model_output\", type=str, default='tanh')\n",
    "parser.add_argument(\"--model_use_batch_norm\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_use_skip\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_use_dual_input\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_dim_feature\", type=int, default=16)\n",
    "parser.add_argument(\"--model_dim_latent\", type=int, default=100)\n",
    "\n",
    "parser.add_argument(\"--data_name\", type=str, default='MNIST')\n",
    "parser.add_argument(\"--data_use_all\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--data_label_subset\", type=int, default=5)\n",
    "parser.add_argument(\"--data_channel\", type=int, default=1)\n",
    "parser.add_argument(\"--data_height\", type=int, default=64)\n",
    "parser.add_argument(\"--data_width\", type=int, default=64)\n",
    "parser.add_argument(\"--data_noise_sigma\", type=float, default=0.15)\n",
    "\n",
    "parser.add_argument(\"--optim_option\", type=str, default='adam')\n",
    "parser.add_argument(\"--optim_length_epoch\", type=int, default=100)\n",
    "parser.add_argument(\"--optim_size_batch\", type=int, default=100)\n",
    "parser.add_argument(\"--optim_lr_model\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_energy\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_data\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_langevin\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--optim_length_langevin\", type=int, default=10)\n",
    "parser.add_argument(\"--optim_weight_gradient\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--optim_weight_regular\", type=float, default=0.0001)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# ======================================================================\n",
    "# assign options\n",
    "# ======================================================================\n",
    "dir_work                = args.dir_work\n",
    "device_cuda             = args.device_cuda\n",
    "\n",
    "model_conv              = args.model_conv\n",
    "model_activation        = args.model_activation\n",
    "model_output            = args.model_output\n",
    "model_use_batch_norm    = args.model_use_batch_norm\n",
    "model_use_skip          = args.model_use_skip\n",
    "model_use_dual_input    = args.model_use_dual_input\n",
    "model_dim_feature       = args.model_dim_feature\n",
    "model_dim_latent        = args.model_dim_latent\n",
    "\n",
    "data_name               = args.data_name.upper()\n",
    "data_use_all            = args.data_use_all\n",
    "data_label_subset       = args.data_label_subset \n",
    "data_channel            = args.data_channel\n",
    "data_height             = args.data_height\n",
    "data_width              = args.data_width\n",
    "data_noise_sigma        = args.data_noise_sigma\n",
    "\n",
    "optim_option            = args.optim_option\n",
    "optim_length_epoch      = args.optim_length_epoch\n",
    "optim_size_batch        = args.optim_size_batch\n",
    "optim_lr_model          = args.optim_lr_model\n",
    "optim_lr_energy         = args.optim_lr_energy\n",
    "optim_lr_data           = args.optim_lr_data\n",
    "optim_lr_langevin       = args.optim_lr_langevin\n",
    "optim_length_langevin   = args.optim_length_langevin\n",
    "optim_weight_gradient   = args.optim_weight_gradient\n",
    "optim_weight_regular    = args.optim_weight_regular\n",
    "\n",
    "# ======================================================================\n",
    "# path for the results\n",
    "# ======================================================================\n",
    "now         = datetime.datetime.now()\n",
    "date_stamp  = now.strftime('%Y_%m_%d') \n",
    "time_stamp  = now.strftime('%H_%M_%S') \n",
    "\n",
    "dir_figure  = os.path.join(dir_work, 'figure')\n",
    "dir_option  = os.path.join(dir_work, 'option')\n",
    "dir_result  = os.path.join(dir_work, 'result')\n",
    "dir_model   = os.path.join(dir_work, 'model')\n",
    "\n",
    "path_figure = os.path.join(dir_figure, data_name)\n",
    "path_option = os.path.join(dir_option, data_name)\n",
    "path_result = os.path.join(dir_result, data_name)\n",
    "path_model  = os.path.join(dir_model, data_name)\n",
    "\n",
    "date_figure = os.path.join(path_figure, date_stamp)\n",
    "date_option = os.path.join(path_option, date_stamp)\n",
    "date_result = os.path.join(path_result, date_stamp)\n",
    "date_model  = os.path.join(path_model, date_stamp)\n",
    "\n",
    "file_figure = os.path.join(date_figure, '{}.png'.format(time_stamp))\n",
    "file_option = os.path.join(date_option, '{}.ini'.format(time_stamp))\n",
    "file_result = os.path.join(date_result, '{}.csv'.format(time_stamp))\n",
    "file_model  = os.path.join(date_model, '{}.pth'.format(time_stamp))\n",
    "\n",
    "if not os.path.exists(dir_figure):\n",
    "    os.mkdir(dir_figure)\n",
    "if not os.path.exists(dir_option):\n",
    "    os.mkdir(dir_option)\n",
    "if not os.path.exists(dir_result):\n",
    "    os.mkdir(dir_result)\n",
    "if not os.path.exists(dir_model):\n",
    "    os.mkdir(dir_model)\n",
    "if not os.path.exists(path_figure):\n",
    "    os.mkdir(path_figure)\n",
    "if not os.path.exists(path_option):\n",
    "    os.mkdir(path_option)\n",
    "if not os.path.exists(path_result):\n",
    "    os.mkdir(path_result)\n",
    "if not os.path.exists(path_model):\n",
    "    os.mkdir(path_model)\n",
    "if not os.path.exists(date_figure):\n",
    "    os.mkdir(date_figure)\n",
    "if not os.path.exists(date_option):\n",
    "    os.mkdir(date_option)\n",
    "if not os.path.exists(date_result):\n",
    "    os.mkdir(date_result)\n",
    "if not os.path.exists(date_model):\n",
    "    os.mkdir(date_model)\n",
    "\n",
    "device = torch.device(f'cuda:{device_cuda}' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# ======================================================================\n",
    "# random seed\n",
    "# ======================================================================\n",
    "pl.seed_everything(0)\n",
    "\n",
    "# ======================================================================\n",
    "# dataset \n",
    "# ======================================================================\n",
    "dir_data = '/nas/users/minhyeok/dataset'\n",
    "\n",
    "transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Resize([data_height, data_width]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Lambda(lambda t: (t - torch.mean(t)) / torch.std(t)) # mean 0, std 1\n",
    "    # torchvision.transforms.Lambda(lambda t: 2.0 * t - 1) \n",
    "])\n",
    "\n",
    "# the name of the dataset is used as upper case\n",
    "if data_name == 'MNIST':\n",
    "    dataset         = torchvision.datasets.MNIST(dir_data, transform=transform, train=True, download=True)\n",
    "    dataset_test    = torchvision.datasets.MNIST(dir_data, transform=transform, train=False, download=True)\n",
    "\n",
    "elif data_name == 'CIFAR10':\n",
    "    dataset                 = torchvision.datasets.CIFAR10(dir_data, transform=transform, train=True, download=True)\n",
    "    dataset.data            = np.array(dataset.data)\n",
    "    dataset.targets         = np.array(dataset.targets)\n",
    "    dataset_test            = torchvision.datasets.CIFAR10(dir_data, transform=transform, train=False, download=True)\n",
    "    dataset_test.data       = np.array(dataset_test.data)\n",
    "    dataset_test.targets    = np.array(dataset_test.targets)\n",
    "\n",
    "elif data_name == 'CELEBA':\n",
    "    dataset = torchvision.datasets.CelebA(dir_data, transform=transform, download=True)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    dataset, dataset_test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "   \n",
    "if data_name == 'MNIST' or data_name == 'CIFAR10': \n",
    "    if not data_use_all:\n",
    "        idx_label               = (dataset.targets==data_label_subset)\n",
    "        dataset.data            = dataset.data[idx_label]\n",
    "        dataset.targets         = dataset.targets[idx_label]\n",
    "        \n",
    "        idx_label               = (dataset_test.targets==data_label_subset)\n",
    "        dataset_test.data       = dataset_test.data[idx_label]\n",
    "        dataset_test.targets    = dataset_test.targets[idx_label]\n",
    "\n",
    "    num_data_real       = len(dataset)\n",
    "    number_data_real    = 5000\n",
    "    dataset.data        = dataset.data[0:number_data_real]\n",
    "    dataset.targets     = dataset.targets[0:number_data_real]\n",
    "\n",
    "dataloader      = torch.utils.data.DataLoader(dataset=dataset, batch_size=optim_size_batch*2, drop_last=True, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=optim_size_batch, drop_last=True, shuffle=True)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# model\n",
    "# ======================================================================\n",
    "model = models.auto_encoder2(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            dim_latent=model_dim_latent,\n",
    "            use_batch_norm=model_use_batch_norm, \n",
    "            activation_output=model_output).to(device)\n",
    "\n",
    "energy = models.energy(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            use_batch_norm=False,\n",
    "            use_dual_input=model_use_dual_input).to(device)\n",
    "\n",
    "\n",
    "if optim_option.lower() == 'sgd':\n",
    "    optim_model     = torch.optim.SGD(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.SGD(energy.parameters(), lr=optim_lr_energy)\n",
    "elif optim_option.lower() == 'adam':\n",
    "    optim_model     = torch.optim.Adam(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.Adam(energy.parameters(), lr=optim_lr_energy)\n",
    "elif optim_option.lower() == 'adamw':\n",
    "    optim_model     = torch.optim.AdamW(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.AdamW(energy.parameters(), lr=optim_lr_energy)\n",
    "\n",
    "\n",
    "scheduler_model     = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_model, factor=0.0001, patience=10, mode='min')\n",
    "scheduler_energy    = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_energy, factor=0.0001, patience=10, mode='min')\n",
    "\n",
    "# ======================================================================\n",
    "# evaluation \n",
    "# ======================================================================\n",
    "psnr = PeakSignalNoiseRatio().to(device)\n",
    "\n",
    "# ======================================================================\n",
    "# training \n",
    "# ======================================================================\n",
    "val_loss_model_mean     = np.zeros(optim_length_epoch)\n",
    "val_loss_model_std      = np.zeros(optim_length_epoch)\n",
    "val_loss_energy_mean    = np.zeros(optim_length_epoch)\n",
    "val_loss_energy_std     = np.zeros(optim_length_epoch)\n",
    "val_psnr_mean           = np.zeros(optim_length_epoch)\n",
    "val_psnr_std            = np.zeros(optim_length_epoch)\n",
    "val_psnr_update_mean    = np.zeros(optim_length_epoch)\n",
    "val_psnr_update_std     = np.zeros(optim_length_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        f = nn.LeakyReLU(0.2)\n",
    "        self.ngf = 128\n",
    "        self.g_batchnorm = False\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, self.ngf*16, 4, 1, 0, bias = True),\n",
    "            nn.BatchNorm2d(self.ngf*4) if self.g_batchnorm else nn.Identity(),\n",
    "            f,\n",
    "\n",
    "            nn.ConvTranspose2d(self.ngf*16, self.ngf*8, 4, 2, 1, bias = True),\n",
    "            nn.BatchNorm2d(self.ngf*4) if self.g_batchnorm else nn.Identity(),\n",
    "            f,\n",
    "\n",
    "            nn.ConvTranspose2d(self.ngf*8, self.ngf*4, 4, 2, 1, bias = True),\n",
    "            nn.BatchNorm2d(self.ngf*4) if self.g_batchnorm else nn.Identity(),\n",
    "            f,\n",
    "\n",
    "            nn.ConvTranspose2d(self.ngf*4, self.ngf*2, 4, 2, 1, bias = True),\n",
    "            nn.BatchNorm2d(self.ngf*2) if self.g_batchnorm else nn.Identity(),\n",
    "            f,\n",
    "\n",
    "            nn.ConvTranspose2d(self.ngf*2, self.ngf*1, 4, 2, 1, bias = True),\n",
    "            nn.BatchNorm2d(self.ngf*1) if self.g_batchnorm else nn.Identity(),\n",
    "            f,\n",
    "\n",
    "            nn.ConvTranspose2d(self.ngf*1, 3, 4, 2, 1),  # in_channel, out_channel, kernel, stride, padding, out_padding\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.gen(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "         Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "    ConvTranspose2d-1      [10, 2048, 4, 4]       3,278,848       3,278,848\n",
      "           Identity-2      [10, 2048, 4, 4]               0               0\n",
      "          LeakyReLU-3      [10, 2048, 4, 4]               0               0\n",
      "    ConvTranspose2d-4      [10, 1024, 8, 8]      33,555,456      33,555,456\n",
      "           Identity-5      [10, 1024, 8, 8]               0               0\n",
      "    ConvTranspose2d-6     [10, 512, 16, 16]       8,389,120       8,389,120\n",
      "           Identity-7     [10, 512, 16, 16]               0               0\n",
      "    ConvTranspose2d-8     [10, 256, 32, 32]       2,097,408       2,097,408\n",
      "           Identity-9     [10, 256, 32, 32]               0               0\n",
      "   ConvTranspose2d-10     [10, 128, 64, 64]         524,416         524,416\n",
      "          Identity-11     [10, 128, 64, 64]               0               0\n",
      "   ConvTranspose2d-12     [10, 3, 128, 128]           6,147           6,147\n",
      "              Tanh-13     [10, 3, 128, 128]               0               0\n",
      "============================================================================\n",
      "Total params: 47,851,395\n",
      "Trainable params: 47,851,395\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "G = _netG()\n",
    "print(mosum.summary(G, torch.zeros(10,100,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.spectral_norm as sn\n",
    "\n",
    "class _netE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.e_sn = False\n",
    "        apply_sn = sn if self.e_sn else lambda x: x\n",
    "\n",
    "        f = nn.LeakyReLU()\n",
    "\n",
    "        self.ebm = nn.Sequential(\n",
    "            apply_sn(nn.Linear(100, 1000)),\n",
    "            f,\n",
    "\n",
    "            apply_sn(nn.Linear(1000, 1000)),\n",
    "            f,\n",
    "\n",
    "            apply_sn(nn.Linear(1000, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.ebm(z.squeeze()).view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1          [10, 1000]         101,000         101,000\n",
      "       LeakyReLU-2          [10, 1000]               0               0\n",
      "          Linear-3          [10, 1000]       1,001,000       1,001,000\n",
      "          Linear-4             [10, 1]           1,001           1,001\n",
      "=======================================================================\n",
      "Total params: 1,103,001\n",
      "Trainable params: 1,103,001\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "G = _netE()\n",
    "\n",
    "print(mosum.summary(G, torch.zeros(10,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x2048 and 512x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_64069/3184530628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0menergy_interp\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0menergy_positive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0menergy_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0menergy_interp\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/users/minhyeok/energy_based_model/code/network_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/users/minhyeok/energy_based_model/code/network_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x2048 and 512x128)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "energy.train()\n",
    "\n",
    "for i in range(optim_length_epoch):\n",
    "    val_loss_model  = []\n",
    "    val_loss_energy = []\n",
    "    val_psnr        = []\n",
    "    val_psnr_update = []\n",
    "\n",
    "    for j, (image, _) in enumerate(dataloader):\n",
    "        data, real  = torch.split(image, optim_size_batch, dim=0)\n",
    "        \n",
    "        noise       = torch.randn_like(data)\n",
    "        data_noise  = data + data_noise_sigma * noise\n",
    "        \n",
    "        noise       = torch.randn_like(data)\n",
    "        real_noise  = real + data_noise_sigma * noise\n",
    "     \n",
    "        data        = data.to(device) \n",
    "        real        = real.to(device) \n",
    "        data_noise  = data_noise.to(device)\n",
    "        real_noise  = real_noise.to(device)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # predictions\n",
    "        # -------------------------------------------------------------------         \n",
    "        pred, mu, log_var, z = model(data_noise)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # interpolation\n",
    "        # -------------------------------------------------------------------         \n",
    "        alpha   = torch.rand(optim_size_batch, 1, 1, 1)\n",
    "        alpha   = alpha.expand_as(real).to(device)\n",
    "        interp  = alpha * real.data + (1 - alpha) * pred.data\n",
    "        interp  = Parameter(interp, requires_grad=True)\n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # predictions\n",
    "        # -------------------------------------------------------------------\n",
    "        if model_use_dual_input:\n",
    "            energy_positive = energy(real_noise, real)        \n",
    "            energy_negative = energy(data_noise, pred)        \n",
    "            energy_interp   = energy(interp, interp)\n",
    "        else:\n",
    "            energy_positive = energy(real)        \n",
    "            energy_negative = energy(pred)        \n",
    "            energy_interp   = energy(interp)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # update energy model \n",
    "        # -------------------------------------------------------------------         \n",
    "        optim_energy.zero_grad()\n",
    "        loss_positive   = energy.compute_loss_positive(energy_negative, energy_positive)\n",
    "        loss_gradient   = losses.compute_gradient_penalty(interp, energy_interp)\n",
    "        loss_energy     = loss_positive + optim_weight_gradient * loss_gradient\n",
    "        loss_energy.backward()\n",
    "        optim_energy.step()\n",
    "        scheduler_energy.step(loss_energy)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # update input fake \n",
    "        # -------------------------------------------------------------------\n",
    "        pred_update = Parameter(pred, requires_grad=True) \n",
    "        \n",
    "        # z = sample_z()\n",
    "        # z = z.clone().detach()\n",
    "        # z.requires_grad = True\n",
    "        for k in range(optim_length_langevin): \n",
    "\n",
    "            if model_use_dual_input:\n",
    "                energy_negative = energy(data_noise, pred_update)\n",
    "            else:\n",
    "                energy_negative = energy(pred_update)\n",
    "            \n",
    "            loss_negative = energy.compute_loss_negative(energy_negative)\n",
    "            loss_negative.backward()\n",
    "            noise = torch.randn_like(pred_update.data)    # N(mean=0, std=1)\n",
    "            pred_update.data = pred_update.data - optim_lr_data * pred_update.grad + optim_lr_langevin * noise\n",
    "            pred_update.grad.detach_()\n",
    "            pred_update.grad.zero_() \n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # update model \n",
    "        # -------------------------------------------------------------------         \n",
    "        optim_model.zero_grad()\n",
    "        pred, mu, log_var, z = model(data_noise) \n",
    "        loss_model, loss_data, loss_regular = model.compute_loss(pred, pred_update.detach(), mu, log_var, optim_weight_regular) \n",
    "        loss_model.backward()\n",
    "        optim_model.step()        \n",
    "        scheduler_model.step(loss_model)\n",
    "\n",
    "        value_psnr          = psnr(pred.data, data).detach().cpu().numpy().mean()\n",
    "        value_psnr_update   = psnr(pred_update.data, data).detach().cpu().numpy().mean()\n",
    "            \n",
    "        val_loss_model.append(loss_model.item()) \n",
    "        val_loss_energy.append(loss_energy.item()) \n",
    "        val_psnr.append(value_psnr)\n",
    "        val_psnr_update.append(value_psnr_update)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        dir_log             = os.path.join(dir_work, 'log')\n",
    "        file_pred           = os.path.join(dir_log, 'image/pred.png')\n",
    "        file_pred_update    = os.path.join(dir_log, 'image/pred_update.png')\n",
    "        # file_pred           = os.path.join(dir_log, 'image/{:03d}.png'.format(i))\n",
    "        # file_pred_update    = os.path.join(dir_log, 'image/{:03d}_update.png'.format(i))\n",
    "        \n",
    "        save_image(pred.data[:25], file_pred, nrow=5, normalize=True)\n",
    "        save_image(pred_update.data[:25], file_pred_update, nrow=5, normalize=True)\n",
    "        \n",
    "    \n",
    "    val_loss_model_mean[i]  = np.mean(val_loss_model)\n",
    "    val_loss_model_std[i]   = np.std(val_loss_model)\n",
    "    val_loss_energy_mean[i] = np.mean(val_loss_energy)\n",
    "    val_loss_energy_std[i]  = np.std(val_loss_energy)\n",
    "    val_psnr_mean[i]        = np.mean(val_psnr)\n",
    "    val_psnr_std[i]         = np.std(val_psnr)\n",
    "    val_psnr_update_mean[i] = np.mean(val_psnr_update)\n",
    "    val_psnr_update_std[i]  = np.std(val_psnr_update)\n",
    "\n",
    "    log = '[%4d/%4d] loss(model)=%8.7f, loss(energy)=%8.7f, psnr=%4.2f, psnr(update)=%4.2f' % (i, optim_length_epoch, \n",
    "                        val_loss_model_mean[i], val_loss_energy_mean[i], val_psnr_mean[i], val_psnr_update_mean[i])\n",
    "    print(log, flush=True)\n",
    "    \n",
    "    if np.isnan(val_loss_model_mean[i]) or np.isnan(val_loss_energy_mean[i]) or val_psnr_mean[i] < 3:\n",
    "        sys.exit('error')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# save the models\n",
    "# -------------------------------------------------------------------          \n",
    "torch.save({\n",
    "    'state_dict_model'      : model.state_dict(),\n",
    "    'state_dict_energy'     : energy.state_dict(),\n",
    "    'model_conv'            : model_conv,\n",
    "    'model_activation'      : model_activation,\n",
    "    'model_output'          : model_output,\n",
    "    'model_use_batch_norm'  : model_use_batch_norm,\n",
    "    'model_use_skip'        : model_use_skip,\n",
    "    'model_use_dual_input'  : model_use_dual_input,\n",
    "    'model_dim_feature'     : model_dim_feature,\n",
    "    'model_dim_latent'      : model_dim_latent,\n",
    "    'data_name'             : data_name,\n",
    "    'data_use_all'          : data_use_all,\n",
    "    'data_label_subset'     : data_label_subset,\n",
    "    'data_channel'          : data_channel,\n",
    "    'data_height'           : data_height,\n",
    "    'data_width'            : data_width,\n",
    "    'data_noise_sigma'      : data_noise_sigma,\n",
    "    'optim_option'          : optim_option,\n",
    "    'optim_length_epoch'    : optim_length_epoch,\n",
    "    'optim_size_batch'      : optim_size_batch,\n",
    "    'optim_lr_model'        : optim_lr_model,\n",
    "    'optim_lr_energy'       : optim_lr_energy,\n",
    "    'optim_lr_data'         : optim_lr_data,\n",
    "    'optim_lr_langevin'     : optim_lr_langevin,\n",
    "    'optim_length_langevin' : optim_length_langevin,\n",
    "    'optim_weight_gradient' : optim_weight_gradient,\n",
    "    'optim_weight_regular'  : optim_weight_regular,\n",
    "}, file_model)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# save the options\n",
    "# -------------------------------------------------------------------         \n",
    "with open(file_option, 'w') as f:\n",
    "    f.write('{}: {}\\n'.format('model_conv', model_conv))\n",
    "    f.write('{}: {}\\n'.format('model_activation', model_activation))\n",
    "    f.write('{}: {}\\n'.format('model_output', model_output))\n",
    "    f.write('{}: {}\\n'.format('model_use_batch_norm', model_use_batch_norm))\n",
    "    f.write('{}: {}\\n'.format('model_use_skip', model_use_skip))\n",
    "    f.write('{}: {}\\n'.format('model_use_dual_input', model_use_dual_input))\n",
    "    f.write('{}: {}\\n'.format('model_dim_feature', model_dim_feature))\n",
    "    f.write('{}: {}\\n'.format('model_dim_latent', model_dim_latent))\n",
    "    f.write('{}: {}\\n'.format('data_name', data_name))\n",
    "    f.write('{}: {}\\n'.format('data_use_all', data_use_all))\n",
    "    f.write('{}: {}\\n'.format('data_label_subset', data_label_subset))\n",
    "    f.write('{}: {}\\n'.format('data_channel', data_channel))\n",
    "    f.write('{}: {}\\n'.format('data_height', data_height))\n",
    "    f.write('{}: {}\\n'.format('data_width', data_width))\n",
    "    f.write('{}: {}\\n'.format('data_noise_sigma', data_noise_sigma))\n",
    "    f.write('{}: {}\\n'.format('optim_option', optim_option))\n",
    "    f.write('{}: {}\\n'.format('optim_length_epoch', optim_length_epoch))\n",
    "    f.write('{}: {}\\n'.format('optim_size_batch', optim_size_batch))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_model', optim_lr_model))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_energy', optim_lr_energy))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_data', optim_lr_data))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_langevin', optim_lr_langevin))\n",
    "    f.write('{}: {}\\n'.format('optim_length_langevin', optim_length_langevin))\n",
    "    f.write('{}: {}\\n'.format('optim_weight_gradient', optim_weight_gradient))\n",
    "    f.write('{}: {}\\n'.format('optim_weight_regular', optim_weight_regular))\n",
    "f.close()\n",
    "    \n",
    "# -------------------------------------------------------------------\n",
    "# save training results\n",
    "# -------------------------------------------------------------------         \n",
    "data        = data.detach().cpu().numpy().squeeze()\n",
    "data_noise  = data_noise.detach().cpu().numpy().squeeze()\n",
    "pred        = pred.detach().cpu().numpy().squeeze()\n",
    "pred_update = pred_update.detach().cpu().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# save training results\n",
    "# -------------------------------------------------------------------         \n",
    "nRow    = 5 \n",
    "nCol    = 5\n",
    "fSize   = 3\n",
    "\n",
    "fig, ax = plt.subplots(nRow, nCol, figsize=(fSize * nCol, fSize * nRow))\n",
    "\n",
    "ax[0][0].set_title('loss (model)')\n",
    "ax[0][0].plot(val_loss_model_mean, color='red')\n",
    "ax[0][0].fill_between(list(range(optim_length_epoch)), val_loss_model_mean-val_loss_model_std, val_loss_model_mean+val_loss_model_std, color='blue', alpha=0.2)\n",
    "\n",
    "ax[0][1].set_title('loss (energy)')\n",
    "ax[0][1].plot(val_loss_energy_mean, color='red')\n",
    "ax[0][1].fill_between(list(range(optim_length_epoch)), val_loss_energy_mean-val_loss_energy_std, val_loss_energy_mean+val_loss_energy_std, color='blue', alpha=0.2)\n",
    "\n",
    "ax[0][2].set_title('psnr')\n",
    "ax[0][2].plot(val_psnr_mean, color='blue', label='y_0')\n",
    "ax[0][2].plot(val_psnr_update_mean, color='red', label='y_n')\n",
    "ax[0][2].legend()\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[1][i].set_title('clean')\n",
    "    ax[1][i].imshow(data[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[2][i].set_title('noisy')\n",
    "    ax[2][i].imshow(data_noise[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[3][i].set_title('y_0')\n",
    "    ax[3][i].imshow(pred[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[4][i].set_title('y_n')\n",
    "    ax[4][i].imshow(pred_update[i], 'gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(file_figure, bbox_inches='tight', dpi=300)\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3035e755443aca2cb3217176319455f49be3dbda138992f9ddc44b43d021598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
