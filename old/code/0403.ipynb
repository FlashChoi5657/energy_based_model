{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os, copy, argparse, configparser\n",
    "import sys, datetime, csv, random\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.utils import save_image\n",
    "import network_models as models\n",
    "import function_losses as losses\n",
    "import pytorch_model_summary as mosum\n",
    "from PIL import Image\n",
    "\n",
    "# ======================================================================\n",
    "# take options \n",
    "# ======================================================================\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dir_work\", type=str, default='/nas/users/minhyeok/energy_based_model')\n",
    "parser.add_argument(\"--device_cuda\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--model_conv\", type=str, default='conv_double_resnet')\n",
    "parser.add_argument(\"--model_activation\", type=str, default='leakyrelu')\n",
    "parser.add_argument(\"--model_output\", type=str, default='sigmoid')\n",
    "parser.add_argument(\"--model_use_batch_norm\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_use_skip\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_use_dual_input\", type=eval, default=True, choices=[True, False])\n",
    "parser.add_argument(\"--model_dim_feature\", type=int, default=16)\n",
    "parser.add_argument(\"--model_dim_latent\", type=int, default=100)\n",
    "\n",
    "parser.add_argument(\"--data_name\", type=str, default='MNIST')\n",
    "parser.add_argument(\"--data_use_all\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--data_label_subset\", type=int, default=5)\n",
    "parser.add_argument(\"--data_channel\", type=int, default=1)\n",
    "parser.add_argument(\"--data_height\", type=int, default=32)\n",
    "parser.add_argument(\"--data_width\", type=int, default=32)\n",
    "parser.add_argument(\"--data_noise_sigma\", type=float, default=0.5)\n",
    "\n",
    "parser.add_argument(\"--optim_option\", type=str, default='adam')\n",
    "parser.add_argument(\"--optim_length_epoch\", type=int, default=100)\n",
    "parser.add_argument(\"--optim_size_batch\", type=int, default=100)\n",
    "parser.add_argument(\"--optim_lr_model\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_energy\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_data\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_langevin\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--optim_length_langevin\", type=int, default=10)\n",
    "parser.add_argument(\"--optim_weight_gradient\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--optim_weight_regular\", type=float, default=0.0001)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# ======================================================================\n",
    "# assign options\n",
    "# ======================================================================\n",
    "dir_work                = args.dir_work\n",
    "device_cuda             = args.device_cuda\n",
    "\n",
    "model_conv              = args.model_conv\n",
    "model_activation        = args.model_activation\n",
    "model_output            = args.model_output\n",
    "model_use_batch_norm    = args.model_use_batch_norm\n",
    "model_use_skip          = args.model_use_skip\n",
    "model_use_dual_input    = args.model_use_dual_input\n",
    "model_dim_feature       = args.model_dim_feature\n",
    "model_dim_latent        = args.model_dim_latent\n",
    "\n",
    "data_name               = args.data_name.upper()\n",
    "data_use_all            = args.data_use_all\n",
    "data_label_subset       = args.data_label_subset \n",
    "data_channel            = args.data_channel\n",
    "data_height             = args.data_height\n",
    "data_width              = args.data_width\n",
    "data_noise_sigma        = args.data_noise_sigma\n",
    "\n",
    "optim_option            = args.optim_option\n",
    "optim_length_epoch      = args.optim_length_epoch\n",
    "optim_size_batch        = args.optim_size_batch\n",
    "optim_lr_model          = args.optim_lr_model\n",
    "optim_lr_energy         = args.optim_lr_energy\n",
    "optim_lr_data           = args.optim_lr_data\n",
    "optim_lr_langevin       = args.optim_lr_langevin\n",
    "optim_length_langevin   = args.optim_length_langevin\n",
    "optim_weight_gradient   = args.optim_weight_gradient\n",
    "optim_weight_regular    = args.optim_weight_regular\n",
    "\n",
    "# ======================================================================\n",
    "# path for the results\n",
    "# ======================================================================\n",
    "now         = datetime.datetime.now()\n",
    "date_stamp  = now.strftime('%Y_%m_%d') \n",
    "time_stamp  = now.strftime('%H_%M_%S') \n",
    "\n",
    "dir_figure  = os.path.join(dir_work, 'figure')\n",
    "dir_option  = os.path.join(dir_work, 'option')\n",
    "dir_result  = os.path.join(dir_work, 'result')\n",
    "dir_model   = os.path.join(dir_work, 'model')\n",
    "\n",
    "path_figure = os.path.join(dir_figure, data_name)\n",
    "path_option = os.path.join(dir_option, data_name)\n",
    "path_result = os.path.join(dir_result, data_name)\n",
    "path_model  = os.path.join(dir_model, data_name)\n",
    "\n",
    "date_figure = os.path.join(path_figure, date_stamp)\n",
    "date_option = os.path.join(path_option, date_stamp)\n",
    "date_result = os.path.join(path_result, date_stamp)\n",
    "date_model  = os.path.join(path_model, date_stamp)\n",
    "\n",
    "file_figure = os.path.join(date_figure, '{}.png'.format(time_stamp))\n",
    "file_option = os.path.join(date_option, '{}.ini'.format(time_stamp))\n",
    "file_result = os.path.join(date_result, '{}.csv'.format(time_stamp))\n",
    "file_model  = os.path.join(date_model, '{}.pth'.format(time_stamp))\n",
    "\n",
    "if not os.path.exists(dir_figure):\n",
    "    os.mkdir(dir_figure)\n",
    "if not os.path.exists(dir_option):\n",
    "    os.mkdir(dir_option)\n",
    "if not os.path.exists(dir_result):\n",
    "    os.mkdir(dir_result)\n",
    "if not os.path.exists(dir_model):\n",
    "    os.mkdir(dir_model)\n",
    "if not os.path.exists(path_figure):\n",
    "    os.mkdir(path_figure)\n",
    "if not os.path.exists(path_option):\n",
    "    os.mkdir(path_option)\n",
    "if not os.path.exists(path_result):\n",
    "    os.mkdir(path_result)\n",
    "if not os.path.exists(path_model):\n",
    "    os.mkdir(path_model)\n",
    "if not os.path.exists(date_figure):\n",
    "    os.mkdir(date_figure)\n",
    "if not os.path.exists(date_option):\n",
    "    os.mkdir(date_option)\n",
    "if not os.path.exists(date_result):\n",
    "    os.mkdir(date_result)\n",
    "if not os.path.exists(date_model):\n",
    "    os.mkdir(date_model)\n",
    "\n",
    "device = torch.device(f'cuda:{device_cuda}' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# ======================================================================\n",
    "# random seed\n",
    "# ======================================================================\n",
    "pl.seed_everything(0)\n",
    "\n",
    "# ======================================================================\n",
    "# dataset \n",
    "# ======================================================================\n",
    "dir_data = '/nas/users/minhyeok/dataset'\n",
    "\n",
    "transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Resize([data_height, data_width]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Lambda(lambda t: (t - torch.mean(t)) / torch.std(t)) # mean 0, std 1\n",
    "    # torchvision.transforms.Lambda(lambda t: 2.0 * t - 1) \n",
    "])\n",
    "\n",
    "# the name of the dataset is used as upper case\n",
    "if data_name == 'MNIST':\n",
    "    dataset         = torchvision.datasets.MNIST(dir_data, transform=transform, train=True, download=True)\n",
    "    dataset_test    = torchvision.datasets.MNIST(dir_data, transform=transform, train=False, download=True)\n",
    "\n",
    "elif data_name == 'CIFAR10':\n",
    "    dataset                 = torchvision.datasets.CIFAR10(dir_data, transform=transform, train=True, download=True)\n",
    "    dataset.data            = np.array(dataset.data)\n",
    "    dataset.targets         = np.array(dataset.targets)\n",
    "    dataset_test            = torchvision.datasets.CIFAR10(dir_data, transform=transform, train=False, download=True)\n",
    "    dataset_test.data       = np.array(dataset_test.data)\n",
    "    dataset_test.targets    = np.array(dataset_test.targets)\n",
    "\n",
    "elif data_name == 'CELEBA':\n",
    "    dataset = torchvision.datasets.CelebA(dir_data, transform=transform, download=True)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    dataset, dataset_test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "   \n",
    "if data_name == 'MNIST' or data_name == 'CIFAR10': \n",
    "    if not data_use_all:\n",
    "        idx_label               = (dataset.targets==data_label_subset)\n",
    "        dataset.data            = dataset.data[idx_label]\n",
    "        dataset.targets         = dataset.targets[idx_label]\n",
    "        \n",
    "        idx_label               = (dataset_test.targets==data_label_subset)\n",
    "        dataset_test.data       = dataset_test.data[idx_label]\n",
    "        dataset_test.targets    = dataset_test.targets[idx_label]\n",
    "\n",
    "    num_data_real       = len(dataset)\n",
    "    number_data_real    = 5000\n",
    "    dataset.data        = dataset.data[0:number_data_real]\n",
    "    dataset.targets     = dataset.targets[0:number_data_real]\n",
    "\n",
    "dataloader      = torch.utils.data.DataLoader(dataset=dataset, batch_size=optim_size_batch*2, drop_last=True, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=optim_size_batch, drop_last=True, shuffle=True)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# model\n",
    "# ======================================================================\n",
    "model = models.auto_encoder2(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            dim_latent=model_dim_latent,\n",
    "            use_batch_norm=model_use_batch_norm, \n",
    "            activation_output=model_output).to(device)\n",
    "'''\n",
    "energy = models.energy2(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            use_batch_norm=model_use_batch_norm,\n",
    "            use_dual_input=model_use_dual_input).to(device)\n",
    "'''\n",
    "\n",
    "energy = models.energy2(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            use_batch_norm=False,\n",
    "            use_dual_input=model_use_dual_input).to(device)\n",
    "\n",
    "\n",
    "if optim_option.lower() == 'sgd':\n",
    "    optim_model     = torch.optim.SGD(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.SGD(energy.parameters(), lr=optim_lr_energy)\n",
    "elif optim_option.lower() == 'adam':\n",
    "    optim_model     = torch.optim.Adam(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.Adam(energy.parameters(), lr=optim_lr_energy)\n",
    "elif optim_option.lower() == 'adamw':\n",
    "    optim_model     = torch.optim.AdamW(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.AdamW(energy.parameters(), lr=optim_lr_energy)\n",
    "\n",
    "\n",
    "scheduler_model     = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_model, factor=0.0001, patience=10, mode='min')\n",
    "scheduler_energy    = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_energy, factor=0.0001, patience=10, mode='min')\n",
    "\n",
    "# ======================================================================\n",
    "# evaluation \n",
    "# ======================================================================\n",
    "psnr = PeakSignalNoiseRatio().to(device)\n",
    "\n",
    "# ======================================================================\n",
    "# training \n",
    "# ======================================================================\n",
    "val_loss_model_mean     = np.zeros(optim_length_epoch)\n",
    "val_loss_model_std      = np.zeros(optim_length_epoch)\n",
    "val_loss_energy_mean    = np.zeros(optim_length_epoch)\n",
    "val_loss_energy_std     = np.zeros(optim_length_epoch)\n",
    "val_psnr_mean           = np.zeros(optim_length_epoch)\n",
    "val_psnr_std            = np.zeros(optim_length_epoch)\n",
    "val_psnr_update_mean    = np.zeros(optim_length_epoch)\n",
    "val_psnr_update_std     = np.zeros(optim_length_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1096]]],\n",
       "\n",
       "\n",
       "        [[[0.9182]]],\n",
       "\n",
       "\n",
       "        [[[0.8143]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(3, 1, 1, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1096, 0.1096],\n",
       "          [0.1096, 0.1096]]],\n",
       "\n",
       "\n",
       "        [[[0.9182, 0.9182],\n",
       "          [0.9182, 0.9182]]],\n",
       "\n",
       "\n",
       "        [[[0.8143, 0.8143],\n",
       "          [0.8143, 0.8143]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.zeros(3,1,2,2)\n",
    "c=a.expand_as(b)\n",
    "c.shape\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0/ 100] loss(model)=0.4490831, loss(energy)=-187903.5072578, psnr=8.23, psnr(update)=6.12\n",
      "[   1/ 100] loss(model)=152.6240607, loss(energy)=-479830.1950000, psnr=10.17, psnr(update)=-12.10\n",
      "[   2/ 100] loss(model)=0.1295332, loss(energy)=-405224.7650000, psnr=10.17, psnr(update)=9.51\n",
      "[   3/ 100] loss(model)=0.1302742, loss(energy)=-407562.4212500, psnr=10.15, psnr(update)=9.50\n",
      "[   4/ 100] loss(model)=0.1302006, loss(energy)=-409165.8437500, psnr=10.15, psnr(update)=9.50\n",
      "[   5/ 100] loss(model)=0.1295956, loss(energy)=-406501.0612500, psnr=10.16, psnr(update)=9.52\n",
      "[   6/ 100] loss(model)=0.1290758, loss(energy)=-405495.8925000, psnr=10.19, psnr(update)=9.52\n",
      "[   7/ 100] loss(model)=0.1285905, loss(energy)=-407001.8837500, psnr=10.16, psnr(update)=9.52\n",
      "[   8/ 100] loss(model)=0.1294566, loss(energy)=-405762.2000000, psnr=10.16, psnr(update)=9.50\n",
      "[   9/ 100] loss(model)=0.1314716, loss(energy)=-413387.9362500, psnr=10.11, psnr(update)=9.48\n",
      "[  10/ 100] loss(model)=0.1303622, loss(energy)=-410417.2425000, psnr=10.14, psnr(update)=9.50\n",
      "[  11/ 100] loss(model)=0.1306449, loss(energy)=-409104.7150000, psnr=10.16, psnr(update)=9.49\n",
      "[  12/ 100] loss(model)=0.1301480, loss(energy)=-407461.1837500, psnr=10.15, psnr(update)=9.49\n",
      "[  13/ 100] loss(model)=0.1288996, loss(energy)=-404479.4162500, psnr=10.19, psnr(update)=9.51\n",
      "[  14/ 100] loss(model)=0.1305999, loss(energy)=-407893.2837500, psnr=10.13, psnr(update)=9.49\n",
      "[  15/ 100] loss(model)=0.1310068, loss(energy)=-410741.8262500, psnr=10.13, psnr(update)=9.48\n",
      "[  16/ 100] loss(model)=0.1293713, loss(energy)=-406987.9775000, psnr=10.14, psnr(update)=9.50\n",
      "[  17/ 100] loss(model)=0.1300426, loss(energy)=-408311.8275000, psnr=10.16, psnr(update)=9.50\n",
      "[  18/ 100] loss(model)=0.1317980, loss(energy)=-414719.8237500, psnr=10.07, psnr(update)=9.47\n",
      "[  19/ 100] loss(model)=0.1300040, loss(energy)=-411859.2225000, psnr=10.12, psnr(update)=9.50\n",
      "[  20/ 100] loss(model)=0.1307396, loss(energy)=-407411.0137500, psnr=10.17, psnr(update)=9.50\n",
      "[  21/ 100] loss(model)=0.1291436, loss(energy)=-407866.1750000, psnr=10.16, psnr(update)=9.50\n",
      "[  22/ 100] loss(model)=0.1302532, loss(energy)=-408103.3250000, psnr=10.13, psnr(update)=9.49\n",
      "[  23/ 100] loss(model)=0.1310168, loss(energy)=-410794.9262500, psnr=10.13, psnr(update)=9.50\n",
      "[  24/ 100] loss(model)=0.1306856, loss(energy)=-411251.8387500, psnr=10.10, psnr(update)=9.49\n",
      "[  25/ 100] loss(model)=0.1293112, loss(energy)=-407361.6500000, psnr=10.16, psnr(update)=9.51\n",
      "[  26/ 100] loss(model)=0.1327843, loss(energy)=-415200.3450000, psnr=10.10, psnr(update)=9.47\n",
      "[  27/ 100] loss(model)=0.1308298, loss(energy)=-411607.6812500, psnr=10.13, psnr(update)=9.50\n",
      "[  28/ 100] loss(model)=0.1310517, loss(energy)=-411005.7075000, psnr=10.11, psnr(update)=9.49\n",
      "[  29/ 100] loss(model)=0.1312651, loss(energy)=-410390.8625000, psnr=10.12, psnr(update)=9.48\n",
      "[  30/ 100] loss(model)=0.1310662, loss(energy)=-410867.4487500, psnr=10.15, psnr(update)=9.50\n",
      "[  31/ 100] loss(model)=0.1302087, loss(energy)=-409162.2562500, psnr=10.11, psnr(update)=9.50\n",
      "[  32/ 100] loss(model)=0.1310167, loss(energy)=-412133.6050000, psnr=10.13, psnr(update)=9.48\n",
      "[  33/ 100] loss(model)=0.1298763, loss(energy)=-408382.4575000, psnr=10.15, psnr(update)=9.49\n",
      "[  34/ 100] loss(model)=0.1301377, loss(energy)=-407698.4662500, psnr=10.13, psnr(update)=9.49\n",
      "[  35/ 100] loss(model)=0.1309687, loss(energy)=-410852.9737500, psnr=10.13, psnr(update)=9.50\n",
      "[  36/ 100] loss(model)=0.1307095, loss(energy)=-409673.2875000, psnr=10.14, psnr(update)=9.50\n",
      "[  37/ 100] loss(model)=0.1319919, loss(energy)=-413966.0350000, psnr=10.12, psnr(update)=9.47\n",
      "[  38/ 100] loss(model)=0.1297374, loss(energy)=-406440.9637500, psnr=10.17, psnr(update)=9.52\n",
      "[  39/ 100] loss(model)=0.1310328, loss(energy)=-409405.8475000, psnr=10.14, psnr(update)=9.49\n",
      "[  40/ 100] loss(model)=0.1306288, loss(energy)=-408574.4387500, psnr=10.15, psnr(update)=9.49\n",
      "[  41/ 100] loss(model)=0.1299171, loss(energy)=-409564.9512500, psnr=10.14, psnr(update)=9.49\n",
      "[  42/ 100] loss(model)=0.1299295, loss(energy)=-408255.1675000, psnr=10.11, psnr(update)=9.49\n",
      "[  43/ 100] loss(model)=0.1297267, loss(energy)=-408421.2900000, psnr=10.14, psnr(update)=9.51\n",
      "[  44/ 100] loss(model)=0.1299214, loss(energy)=-407498.8487500, psnr=10.16, psnr(update)=9.51\n",
      "[  45/ 100] loss(model)=0.1318936, loss(energy)=-409972.9400000, psnr=10.13, psnr(update)=9.47\n",
      "[  46/ 100] loss(model)=0.1321711, loss(energy)=-408859.8700000, psnr=10.15, psnr(update)=9.49\n",
      "[  47/ 100] loss(model)=0.1301986, loss(energy)=-407977.5000000, psnr=10.15, psnr(update)=9.48\n",
      "[  48/ 100] loss(model)=0.1309313, loss(energy)=-407952.6412500, psnr=10.13, psnr(update)=9.48\n",
      "[  49/ 100] loss(model)=0.1305414, loss(energy)=-408207.8387500, psnr=10.15, psnr(update)=9.49\n",
      "[  50/ 100] loss(model)=0.1300152, loss(energy)=-406692.3525000, psnr=10.15, psnr(update)=9.50\n",
      "[  51/ 100] loss(model)=0.1311688, loss(energy)=-411575.5425000, psnr=10.11, psnr(update)=9.48\n",
      "[  52/ 100] loss(model)=0.1300421, loss(energy)=-405528.4562500, psnr=10.17, psnr(update)=9.50\n",
      "[  53/ 100] loss(model)=0.1313571, loss(energy)=-412608.6050000, psnr=10.12, psnr(update)=9.48\n",
      "[  54/ 100] loss(model)=0.1310063, loss(energy)=-411306.5687500, psnr=10.11, psnr(update)=9.48\n",
      "[  55/ 100] loss(model)=0.1310032, loss(energy)=-411902.1712500, psnr=10.16, psnr(update)=9.51\n",
      "[  56/ 100] loss(model)=0.1300917, loss(energy)=-407745.7625000, psnr=10.15, psnr(update)=9.50\n",
      "[  57/ 100] loss(model)=0.1309803, loss(energy)=-410765.7462500, psnr=10.13, psnr(update)=9.48\n",
      "[  58/ 100] loss(model)=0.1317299, loss(energy)=-413149.5600000, psnr=10.10, psnr(update)=9.46\n",
      "[  59/ 100] loss(model)=0.1315793, loss(energy)=-412412.7162500, psnr=10.10, psnr(update)=9.46\n",
      "[  60/ 100] loss(model)=0.1309296, loss(energy)=-410128.8625000, psnr=10.15, psnr(update)=9.50\n",
      "[  61/ 100] loss(model)=0.1306605, loss(energy)=-410523.9475000, psnr=10.13, psnr(update)=9.49\n",
      "[  62/ 100] loss(model)=0.1305071, loss(energy)=-411263.1862500, psnr=10.10, psnr(update)=9.47\n",
      "[  63/ 100] loss(model)=0.1296306, loss(energy)=-408827.4312500, psnr=10.17, psnr(update)=9.50\n",
      "[  64/ 100] loss(model)=0.1310051, loss(energy)=-408518.2512500, psnr=10.15, psnr(update)=9.50\n",
      "[  65/ 100] loss(model)=0.1303076, loss(energy)=-408377.7187500, psnr=10.15, psnr(update)=9.52\n",
      "[  66/ 100] loss(model)=0.1297135, loss(energy)=-409876.9637500, psnr=10.16, psnr(update)=9.51\n",
      "[  67/ 100] loss(model)=0.1301850, loss(energy)=-409142.8450000, psnr=10.14, psnr(update)=9.51\n",
      "[  68/ 100] loss(model)=0.1303333, loss(energy)=-410720.0512500, psnr=10.14, psnr(update)=9.51\n",
      "[  69/ 100] loss(model)=0.1308442, loss(energy)=-408321.7887500, psnr=10.17, psnr(update)=9.50\n",
      "[  70/ 100] loss(model)=0.1304653, loss(energy)=-409419.2262500, psnr=10.11, psnr(update)=9.49\n",
      "[  71/ 100] loss(model)=0.1290786, loss(energy)=-406519.2762500, psnr=10.15, psnr(update)=9.51\n",
      "[  72/ 100] loss(model)=0.1314597, loss(energy)=-409218.8087500, psnr=10.12, psnr(update)=9.47\n",
      "[  73/ 100] loss(model)=0.1297880, loss(energy)=-409961.8800000, psnr=10.13, psnr(update)=9.50\n",
      "[  74/ 100] loss(model)=0.1307694, loss(energy)=-406796.5362500, psnr=10.14, psnr(update)=9.48\n",
      "[  75/ 100] loss(model)=0.1294255, loss(energy)=-407994.8200000, psnr=10.16, psnr(update)=9.51\n",
      "[  76/ 100] loss(model)=0.1313771, loss(energy)=-410709.3150000, psnr=10.11, psnr(update)=9.47\n",
      "[  77/ 100] loss(model)=0.1303958, loss(energy)=-410013.1825000, psnr=10.12, psnr(update)=9.47\n",
      "[  78/ 100] loss(model)=0.1297175, loss(energy)=-409541.5450000, psnr=10.12, psnr(update)=9.50\n",
      "[  79/ 100] loss(model)=0.1308720, loss(energy)=-410136.4725000, psnr=10.10, psnr(update)=9.47\n",
      "[  80/ 100] loss(model)=0.1305097, loss(energy)=-409190.7812500, psnr=10.13, psnr(update)=9.49\n",
      "[  81/ 100] loss(model)=0.1286879, loss(energy)=-406252.2100000, psnr=10.17, psnr(update)=9.50\n",
      "[  82/ 100] loss(model)=0.1291542, loss(energy)=-407084.8000000, psnr=10.14, psnr(update)=9.50\n",
      "[  83/ 100] loss(model)=0.1322511, loss(energy)=-412972.4975000, psnr=10.09, psnr(update)=9.46\n",
      "[  84/ 100] loss(model)=0.1300121, loss(energy)=-407651.0650000, psnr=10.15, psnr(update)=9.49\n",
      "[  85/ 100] loss(model)=0.1288474, loss(energy)=-404911.9712500, psnr=10.18, psnr(update)=9.53\n",
      "[  86/ 100] loss(model)=0.1303957, loss(energy)=-409770.8137500, psnr=10.16, psnr(update)=9.51\n",
      "[  87/ 100] loss(model)=0.1292616, loss(energy)=-406578.7575000, psnr=10.16, psnr(update)=9.51\n",
      "[  88/ 100] loss(model)=0.1307514, loss(energy)=-411669.4562500, psnr=10.13, psnr(update)=9.49\n",
      "[  89/ 100] loss(model)=0.1315451, loss(energy)=-409131.5950000, psnr=10.13, psnr(update)=9.47\n",
      "[  90/ 100] loss(model)=0.1297725, loss(energy)=-407915.5850000, psnr=10.13, psnr(update)=9.50\n",
      "[  91/ 100] loss(model)=0.1306138, loss(energy)=-410492.3550000, psnr=10.12, psnr(update)=9.48\n",
      "[  92/ 100] loss(model)=0.1300413, loss(energy)=-410779.9562500, psnr=10.12, psnr(update)=9.49\n",
      "[  93/ 100] loss(model)=0.1306545, loss(energy)=-409525.5500000, psnr=10.13, psnr(update)=9.49\n",
      "[  94/ 100] loss(model)=0.1305347, loss(energy)=-412802.9400000, psnr=10.10, psnr(update)=9.49\n",
      "[  95/ 100] loss(model)=0.1307595, loss(energy)=-409439.2325000, psnr=10.13, psnr(update)=9.49\n",
      "[  96/ 100] loss(model)=0.1302871, loss(energy)=-407582.5075000, psnr=10.15, psnr(update)=9.48\n",
      "[  97/ 100] loss(model)=0.1307585, loss(energy)=-410937.6325000, psnr=10.09, psnr(update)=9.47\n",
      "[  98/ 100] loss(model)=0.1301626, loss(energy)=-406657.4450000, psnr=10.15, psnr(update)=9.48\n",
      "[  99/ 100] loss(model)=0.1300945, loss(energy)=-406675.5375000, psnr=10.17, psnr(update)=9.50\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "energy.train()\n",
    "\n",
    "for i in range(optim_length_epoch):\n",
    "    val_loss_model  = []\n",
    "    val_loss_energy = []\n",
    "    val_psnr        = []\n",
    "    val_psnr_update = []\n",
    "\n",
    "    for j, (image, _) in enumerate(dataloader):\n",
    "        data, real  = torch.split(image, optim_size_batch, dim=0)\n",
    "        \n",
    "        noise       = torch.randn_like(data)\n",
    "        data_noise  = data + data_noise_sigma * noise\n",
    "        \n",
    "        noise       = torch.randn_like(data)\n",
    "        real_noise  = real + data_noise_sigma * noise\n",
    "     \n",
    "        data        = data.to(device) \n",
    "        real        = real.to(device) \n",
    "        data_noise  = data_noise.to(device)\n",
    "        real_noise  = real_noise.to(device)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # predictions\n",
    "        # -------------------------------------------------------------------         \n",
    "        pred, mu, log_var, z = model(data_noise)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # interpolation\n",
    "        # -------------------------------------------------------------------         \n",
    "        alpha   = torch.rand(optim_size_batch, 1, 1, 1)\n",
    "        alpha   = alpha.expand_as(real).to(device)\n",
    "        interp  = alpha * real.data + (1 - alpha) * pred.data\n",
    "        interp  = Parameter(interp, requires_grad=True)\n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # predictions\n",
    "        # -------------------------------------------------------------------\n",
    "        if model_use_dual_input:\n",
    "            energy_positive = energy(real_noise, real)        \n",
    "            energy_negative = energy(data_noise, pred)        \n",
    "            energy_interp   = energy(interp, interp)\n",
    "        else:\n",
    "            energy_positive = energy(real)        \n",
    "            energy_negative = energy(pred)        \n",
    "            energy_interp   = energy(interp)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # update energy model \n",
    "        # -------------------------------------------------------------------         \n",
    "        optim_energy.zero_grad()\n",
    "        loss_positive   = energy.compute_loss_positive(energy_negative, energy_positive)\n",
    "        loss_gradient   = losses.compute_gradient_penalty(interp, energy_interp)\n",
    "        loss_energy     = loss_positive + optim_weight_gradient * loss_gradient\n",
    "        loss_energy.backward()\n",
    "        optim_energy.step()\n",
    "        scheduler_energy.step(loss_energy)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # update input fake \n",
    "        # -------------------------------------------------------------------\n",
    "        pred_update = Parameter(pred, requires_grad=True) \n",
    "        \n",
    "        for k in range(optim_length_langevin): \n",
    "\n",
    "            if model_use_dual_input:\n",
    "                energy_negative = energy(data_noise, pred_update)\n",
    "            else:\n",
    "                energy_negative = energy(pred_update)\n",
    "                \n",
    "            loss_negative = energy.compute_loss_negative(energy_negative)\n",
    "            loss_negative.backward()\n",
    "            noise = torch.randn_like(pred_update.data)    # N(mean=0, std=1)\n",
    "            pred_update.data = pred_update.data - optim_lr_data * pred_update.grad + optim_lr_langevin * noise\n",
    "            pred_update.grad.detach_()\n",
    "            pred_update.grad.zero_() \n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # update model \n",
    "        # -------------------------------------------------------------------         \n",
    "        optim_model.zero_grad()\n",
    "        pred, mu, log_var, z = model(data_noise) \n",
    "        loss_model, loss_data, loss_regular = model.compute_loss(pred, pred_update.detach(), mu, log_var, optim_weight_regular) \n",
    "        loss_model.backward()\n",
    "        optim_model.step()        \n",
    "        scheduler_model.step(loss_model)\n",
    "\n",
    "        value_psnr          = psnr(pred.data, data).detach().cpu().numpy().mean()\n",
    "        value_psnr_update   = psnr(pred_update.data, data).detach().cpu().numpy().mean()\n",
    "            \n",
    "        val_loss_model.append(loss_model.item()) \n",
    "        val_loss_energy.append(loss_energy.item()) \n",
    "        val_psnr.append(value_psnr)\n",
    "        val_psnr_update.append(value_psnr_update)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        dir_log             = os.path.join(dir_work, 'log')\n",
    "        file_pred           = os.path.join(dir_log, 'image/pred.png')\n",
    "        file_pred_update    = os.path.join(dir_log, 'image/pred_update.png')\n",
    "        # file_pred           = os.path.join(dir_log, 'image/{:03d}.png'.format(i))\n",
    "        # file_pred_update    = os.path.join(dir_log, 'image/{:03d}_update.png'.format(i))\n",
    "        \n",
    "        save_image(pred.data[:25], file_pred, nrow=5, normalize=True)\n",
    "        save_image(pred_update.data[:25], file_pred_update, nrow=5, normalize=True)\n",
    "        \n",
    "    \n",
    "    val_loss_model_mean[i]  = np.mean(val_loss_model)\n",
    "    val_loss_model_std[i]   = np.std(val_loss_model)\n",
    "    val_loss_energy_mean[i] = np.mean(val_loss_energy)\n",
    "    val_loss_energy_std[i]  = np.std(val_loss_energy)\n",
    "    val_psnr_mean[i]        = np.mean(val_psnr)\n",
    "    val_psnr_std[i]         = np.std(val_psnr)\n",
    "    val_psnr_update_mean[i] = np.mean(val_psnr_update)\n",
    "    val_psnr_update_std[i]  = np.std(val_psnr_update)\n",
    "\n",
    "    log = '[%4d/%4d] loss(model)=%8.7f, loss(energy)=%8.7f, psnr=%4.2f, psnr(update)=%4.2f' % (i, optim_length_epoch, val_loss_model_mean[i], val_loss_energy_mean[i], val_psnr_mean[i], val_psnr_update_mean[i])\n",
    "    print(log, flush=True)\n",
    "    \n",
    "    if np.isnan(val_loss_model_mean[i]) or np.isnan(val_loss_energy_mean[i]) or val_psnr_mean[i] < 3:\n",
    "        sys.exit('error')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# save the models\n",
    "# -------------------------------------------------------------------          \n",
    "torch.save({\n",
    "    'state_dict_model'      : model.state_dict(),\n",
    "    'state_dict_energy'     : energy.state_dict(),\n",
    "    'model_conv'            : model_conv,\n",
    "    'model_activation'      : model_activation,\n",
    "    'model_output'          : model_output,\n",
    "    'model_use_batch_norm'  : model_use_batch_norm,\n",
    "    'model_use_skip'        : model_use_skip,\n",
    "    'model_use_dual_input'  : model_use_dual_input,\n",
    "    'model_dim_feature'     : model_dim_feature,\n",
    "    'model_dim_latent'      : model_dim_latent,\n",
    "    'data_name'             : data_name,\n",
    "    'data_use_all'          : data_use_all,\n",
    "    'data_label_subset'     : data_label_subset,\n",
    "    'data_channel'          : data_channel,\n",
    "    'data_height'           : data_height,\n",
    "    'data_width'            : data_width,\n",
    "    'data_noise_sigma'      : data_noise_sigma,\n",
    "    'optim_option'          : optim_option,\n",
    "    'optim_length_epoch'    : optim_length_epoch,\n",
    "    'optim_size_batch'      : optim_size_batch,\n",
    "    'optim_lr_model'        : optim_lr_model,\n",
    "    'optim_lr_energy'       : optim_lr_energy,\n",
    "    'optim_lr_data'         : optim_lr_data,\n",
    "    'optim_lr_langevin'     : optim_lr_langevin,\n",
    "    'optim_length_langevin' : optim_length_langevin,\n",
    "    'optim_weight_gradient' : optim_weight_gradient,\n",
    "    'optim_weight_regular'  : optim_weight_regular,\n",
    "}, file_model)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# save the options\n",
    "# -------------------------------------------------------------------         \n",
    "with open(file_option, 'w') as f:\n",
    "    f.write('{}: {}\\n'.format('model_conv', model_conv))\n",
    "    f.write('{}: {}\\n'.format('model_activation', model_activation))\n",
    "    f.write('{}: {}\\n'.format('model_output', model_output))\n",
    "    f.write('{}: {}\\n'.format('model_use_batch_norm', model_use_batch_norm))\n",
    "    f.write('{}: {}\\n'.format('model_use_skip', model_use_skip))\n",
    "    f.write('{}: {}\\n'.format('model_use_dual_input', model_use_dual_input))\n",
    "    f.write('{}: {}\\n'.format('model_dim_feature', model_dim_feature))\n",
    "    f.write('{}: {}\\n'.format('model_dim_latent', model_dim_latent))\n",
    "    f.write('{}: {}\\n'.format('data_name', data_name))\n",
    "    f.write('{}: {}\\n'.format('data_use_all', data_use_all))\n",
    "    f.write('{}: {}\\n'.format('data_label_subset', data_label_subset))\n",
    "    f.write('{}: {}\\n'.format('data_channel', data_channel))\n",
    "    f.write('{}: {}\\n'.format('data_height', data_height))\n",
    "    f.write('{}: {}\\n'.format('data_width', data_width))\n",
    "    f.write('{}: {}\\n'.format('data_noise_sigma', data_noise_sigma))\n",
    "    f.write('{}: {}\\n'.format('optim_option', optim_option))\n",
    "    f.write('{}: {}\\n'.format('optim_length_epoch', optim_length_epoch))\n",
    "    f.write('{}: {}\\n'.format('optim_size_batch', optim_size_batch))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_model', optim_lr_model))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_energy', optim_lr_energy))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_data', optim_lr_data))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_langevin', optim_lr_langevin))\n",
    "    f.write('{}: {}\\n'.format('optim_length_langevin', optim_length_langevin))\n",
    "    f.write('{}: {}\\n'.format('optim_weight_gradient', optim_weight_gradient))\n",
    "    f.write('{}: {}\\n'.format('optim_weight_regular', optim_weight_regular))\n",
    "f.close()\n",
    "    \n",
    "# -------------------------------------------------------------------\n",
    "# save training results\n",
    "# -------------------------------------------------------------------         \n",
    "data        = data.detach().cpu().numpy().squeeze()\n",
    "data_noise  = data_noise.detach().cpu().numpy().squeeze()\n",
    "pred        = pred.detach().cpu().numpy().squeeze()\n",
    "pred_update = pred_update.detach().cpu().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# save training results\n",
    "# -------------------------------------------------------------------         \n",
    "nRow    = 5 \n",
    "nCol    = 5\n",
    "fSize   = 3\n",
    "\n",
    "fig, ax = plt.subplots(nRow, nCol, figsize=(fSize * nCol, fSize * nRow))\n",
    "\n",
    "ax[0][0].set_title('loss (model)')\n",
    "ax[0][0].plot(val_loss_model_mean, color='red')\n",
    "ax[0][0].fill_between(list(range(optim_length_epoch)), val_loss_model_mean-val_loss_model_std, val_loss_model_mean+val_loss_model_std, color='blue', alpha=0.2)\n",
    "\n",
    "ax[0][1].set_title('loss (energy)')\n",
    "ax[0][1].plot(val_loss_energy_mean, color='red')\n",
    "ax[0][1].fill_between(list(range(optim_length_epoch)), val_loss_energy_mean-val_loss_energy_std, val_loss_energy_mean+val_loss_energy_std, color='blue', alpha=0.2)\n",
    "\n",
    "ax[0][2].set_title('psnr')\n",
    "ax[0][2].plot(val_psnr_mean, color='blue', label='y_0')\n",
    "ax[0][2].plot(val_psnr_update_mean, color='red', label='y_n')\n",
    "ax[0][2].legend()\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[1][i].set_title('clean')\n",
    "    ax[1][i].imshow(data[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[2][i].set_title('noisy')\n",
    "    ax[2][i].imshow(data_noise[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[3][i].set_title('y_0')\n",
    "    ax[3][i].imshow(pred[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[4][i].set_title('y_n')\n",
    "    ax[4][i].imshow(pred_update[i], 'gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(file_figure, bbox_inches='tight', dpi=300)\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3035e755443aca2cb3217176319455f49be3dbda138992f9ddc44b43d021598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
