{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os, copy, argparse, configparser\n",
    "import sys, datetime, csv, random\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.utils import save_image\n",
    "import network_models as models\n",
    "import function_losses as losses\n",
    "import pytorch_model_summary as mosum\n",
    "from PIL import Image\n",
    "\n",
    "# ======================================================================\n",
    "# take options \n",
    "# ======================================================================\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dir_work\", type=str, default='/nas/users/minhyeok/energy_based_model')\n",
    "parser.add_argument(\"--device_cuda\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--model_conv\", type=str, default='conv_double_resnet')\n",
    "parser.add_argument(\"--model_activation\", type=str, default='leakyrelu')\n",
    "parser.add_argument(\"--model_output\", type=str, default='tanh')\n",
    "parser.add_argument(\"--model_use_batch_norm\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_use_skip\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_use_dual_input\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--model_dim_feature\", type=int, default=16)\n",
    "parser.add_argument(\"--model_dim_latent\", type=int, default=100)\n",
    "\n",
    "parser.add_argument(\"--data_name\", type=str, default='MNIST')\n",
    "parser.add_argument(\"--data_use_all\", type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument(\"--data_label_subset\", type=int, default=5)\n",
    "parser.add_argument(\"--data_channel\", type=int, default=1)\n",
    "parser.add_argument(\"--image_size\", type=int, default=32)\n",
    "parser.add_argument(\"--data_noise_sigma\", type=float, default=0.15)\n",
    "\n",
    "parser.add_argument(\"--optim_option\", type=str, default='adam')\n",
    "parser.add_argument(\"--optim_length_epoch\", type=int, default=510)\n",
    "parser.add_argument(\"--optim_size_batch\", type=int, default=100)\n",
    "parser.add_argument(\"--optim_lr_model\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_energy\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_data\", type=float, default=0.01)\n",
    "parser.add_argument(\"--optim_lr_langevin\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--optim_length_langevin\", type=int, default=10)\n",
    "parser.add_argument(\"--optim_weight_gradient\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--optim_weight_regular\", type=float, default=0.0001)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# ======================================================================\n",
    "# assign options\n",
    "# ======================================================================\n",
    "dir_work                = args.dir_work\n",
    "device_cuda             = args.device_cuda\n",
    "\n",
    "model_conv              = args.model_conv\n",
    "model_activation        = args.model_activation\n",
    "model_output            = args.model_output\n",
    "model_use_batch_norm    = args.model_use_batch_norm\n",
    "model_use_skip          = args.model_use_skip\n",
    "model_use_dual_input    = args.model_use_dual_input\n",
    "model_dim_feature       = args.model_dim_feature\n",
    "model_dim_latent        = args.model_dim_latent\n",
    "\n",
    "data_name               = args.data_name.upper()\n",
    "data_use_all            = args.data_use_all\n",
    "data_label_subset       = args.data_label_subset \n",
    "data_channel            = args.data_channel\n",
    "img_size                = args.image_size\n",
    "data_noise_sigma        = args.data_noise_sigma\n",
    "\n",
    "optim_option            = args.optim_option\n",
    "optim_length_epoch      = args.optim_length_epoch\n",
    "optim_size_batch        = args.optim_size_batch\n",
    "optim_lr_model          = args.optim_lr_model\n",
    "optim_lr_energy         = args.optim_lr_energy\n",
    "optim_lr_data           = args.optim_lr_data\n",
    "optim_lr_langevin       = args.optim_lr_langevin\n",
    "optim_length_langevin   = args.optim_length_langevin\n",
    "optim_weight_gradient   = args.optim_weight_gradient\n",
    "optim_weight_regular    = args.optim_weight_regular\n",
    "\n",
    "# ======================================================================\n",
    "# path for the results\n",
    "# ======================================================================\n",
    "now         = datetime.datetime.now()\n",
    "date_stamp  = now.strftime('%Y_%m_%d') \n",
    "time_stamp  = now.strftime('%H_%M_%S') \n",
    "\n",
    "dir_figure  = os.path.join(dir_work, 'figure')\n",
    "dir_option  = os.path.join(dir_work, 'option')\n",
    "dir_result  = os.path.join(dir_work, 'result')\n",
    "dir_model   = os.path.join(dir_work, 'model')\n",
    "\n",
    "path_figure = os.path.join(dir_figure, data_name)\n",
    "path_option = os.path.join(dir_option, data_name)\n",
    "path_result = os.path.join(dir_result, data_name)\n",
    "path_model  = os.path.join(dir_model, data_name)\n",
    "\n",
    "date_figure = os.path.join(path_figure, date_stamp)\n",
    "date_option = os.path.join(path_option, date_stamp)\n",
    "date_result = os.path.join(path_result, date_stamp)\n",
    "date_model  = os.path.join(path_model, date_stamp)\n",
    "\n",
    "file_figure = os.path.join(date_figure, '{}.png'.format(time_stamp))\n",
    "file_option = os.path.join(date_option, '{}.ini'.format(time_stamp))\n",
    "file_result = os.path.join(date_result, '{}.csv'.format(time_stamp))\n",
    "file_model  = os.path.join(date_model, '{}.pth'.format(time_stamp))\n",
    "\n",
    "if not os.path.exists(dir_figure):\n",
    "    os.mkdir(dir_figure)\n",
    "if not os.path.exists(dir_option):\n",
    "    os.mkdir(dir_option)\n",
    "if not os.path.exists(dir_result):\n",
    "    os.mkdir(dir_result)\n",
    "if not os.path.exists(dir_model):\n",
    "    os.mkdir(dir_model)\n",
    "if not os.path.exists(path_figure):\n",
    "    os.mkdir(path_figure)\n",
    "if not os.path.exists(path_option):\n",
    "    os.mkdir(path_option)\n",
    "if not os.path.exists(path_result):\n",
    "    os.mkdir(path_result)\n",
    "if not os.path.exists(path_model):\n",
    "    os.mkdir(path_model)\n",
    "if not os.path.exists(date_figure):\n",
    "    os.mkdir(date_figure)\n",
    "if not os.path.exists(date_option):\n",
    "    os.mkdir(date_option)\n",
    "if not os.path.exists(date_result):\n",
    "    os.mkdir(date_result)\n",
    "if not os.path.exists(date_model):\n",
    "    os.mkdir(date_model)\n",
    "\n",
    "device = torch.device(f'cuda:{device_cuda}' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# ======================================================================\n",
    "# random seed\n",
    "# ======================================================================\n",
    "pl.seed_everything(0)\n",
    "\n",
    "# ======================================================================\n",
    "# dataset \n",
    "# ======================================================================\n",
    "dir_data = '/nas/users/minhyeok/dataset'\n",
    "\n",
    "transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Resize(img_size),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Lambda(lambda t: (t - torch.mean(t)) / torch.std(t)) # mean 0, std 1\n",
    "    # torchvision.transforms.Lambda(lambda t: 2.0 * t - 1) \n",
    "])\n",
    "\n",
    "# the name of the dataset is used as upper case\n",
    "if data_name == 'MNIST':\n",
    "    dataset         = torchvision.datasets.MNIST(dir_data, transform=transform, train=True, download=True)\n",
    "    dataset_test    = torchvision.datasets.MNIST(dir_data, transform=transform, train=False, download=True)\n",
    "\n",
    "elif data_name == 'CIFAR10':\n",
    "    dataset                 = torchvision.datasets.CIFAR10(dir_data, transform=transform, train=True, download=True)\n",
    "    dataset.data            = np.array(dataset.data)\n",
    "    dataset.targets         = np.array(dataset.targets)\n",
    "    dataset_test            = torchvision.datasets.CIFAR10(dir_data, transform=transform, train=False, download=True)\n",
    "    dataset_test.data       = np.array(dataset_test.data)\n",
    "    dataset_test.targets    = np.array(dataset_test.targets)\n",
    "\n",
    "elif data_name == 'CELEBA':\n",
    "    dataset = torchvision.datasets.CelebA(dir_data, transform=transform, download=True)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    dataset, dataset_test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "   \n",
    "if data_name == 'MNIST' or data_name == 'CIFAR10': \n",
    "    if not data_use_all:\n",
    "        idx_label               = (dataset.targets==data_label_subset)\n",
    "        dataset.data            = dataset.data[idx_label]\n",
    "        dataset.targets         = dataset.targets[idx_label]\n",
    "        \n",
    "        idx_label               = (dataset_test.targets==data_label_subset)\n",
    "        dataset_test.data       = dataset_test.data[idx_label]\n",
    "        dataset_test.targets    = dataset_test.targets[idx_label]\n",
    "\n",
    "    num_data_real       = len(dataset)\n",
    "    number_data_real    = 5000\n",
    "    dataset.data        = dataset.data[0:number_data_real]\n",
    "    dataset.targets     = dataset.targets[0:number_data_real]\n",
    "\n",
    "dataloader      = torch.utils.data.DataLoader(dataset=dataset, batch_size=optim_size_batch*2, drop_last=True, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=optim_size_batch, drop_last=True, shuffle=True)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# model\n",
    "# ======================================================================\n",
    "model = models.auto_encoder2(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            dim_latent=model_dim_latent,\n",
    "            use_batch_norm=model_use_batch_norm, \n",
    "            activation_output=model_output).to(device)\n",
    "\n",
    "energy = models.energy(\n",
    "            dim_channel=data_channel,\n",
    "            dim_feature=model_dim_feature,\n",
    "            dim_latent=model_dim_latent,\n",
    "            use_batch_norm=False,\n",
    "            use_dual_input=model_use_dual_input).to(device)\n",
    "\n",
    "\n",
    "if optim_option.lower() == 'sgd':\n",
    "    optim_model     = torch.optim.SGD(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.SGD(energy.parameters(), lr=optim_lr_energy)\n",
    "elif optim_option.lower() == 'adam':\n",
    "    optim_model     = torch.optim.Adam(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.Adam(energy.parameters(), lr=optim_lr_energy)\n",
    "elif optim_option.lower() == 'adamw':\n",
    "    optim_model     = torch.optim.AdamW(model.parameters(), lr=optim_lr_model)\n",
    "    optim_energy    = torch.optim.AdamW(energy.parameters(), lr=optim_lr_energy)\n",
    "\n",
    "\n",
    "scheduler_model     = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_model, factor=0.0001, patience=10, mode='min')\n",
    "scheduler_energy    = torch.optim.lr_scheduler.ReduceLROnPlateau(optim_energy, factor=0.0001, patience=10, mode='min')\n",
    "\n",
    "# ======================================================================\n",
    "# evaluation \n",
    "# ======================================================================\n",
    "psnr = PeakSignalNoiseRatio().to(device)\n",
    "\n",
    "# ======================================================================\n",
    "# training \n",
    "# ======================================================================\n",
    "val_loss_model_mean     = np.zeros(optim_length_epoch)\n",
    "val_loss_model_std      = np.zeros(optim_length_epoch)\n",
    "val_loss_energy_mean    = np.zeros(optim_length_epoch)\n",
    "val_loss_energy_std     = np.zeros(optim_length_epoch)\n",
    "val_psnr_mean           = np.zeros(optim_length_epoch)\n",
    "val_psnr_std            = np.zeros(optim_length_epoch)\n",
    "val_psnr_update_mean    = np.zeros(optim_length_epoch)\n",
    "val_psnr_update_std     = np.zeros(optim_length_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0/ 510] loss(model)=0.0000325, loss(energy)=213015651.8400000, psnr=10.48, psnr(update)=10.48\n",
      "[   1/ 510] loss(model)=0.0000325, loss(energy)=215792788.4800000, psnr=10.47, psnr(update)=10.47\n",
      "[   2/ 510] loss(model)=0.0000326, loss(energy)=210396989.4400000, psnr=10.46, psnr(update)=10.46\n",
      "[   3/ 510] loss(model)=0.0000309, loss(energy)=214358992.6400000, psnr=10.51, psnr(update)=10.51\n",
      "[   4/ 510] loss(model)=0.0000335, loss(energy)=213903562.2400000, psnr=10.49, psnr(update)=10.49\n",
      "[   5/ 510] loss(model)=0.0000348, loss(energy)=210973778.5600000, psnr=10.49, psnr(update)=10.49\n",
      "[   6/ 510] loss(model)=0.0000339, loss(energy)=213998482.5600000, psnr=10.46, psnr(update)=10.46\n",
      "[   7/ 510] loss(model)=0.0000337, loss(energy)=211869769.6000000, psnr=10.46, psnr(update)=10.46\n",
      "[   8/ 510] loss(model)=0.0000318, loss(energy)=211450182.4000000, psnr=10.43, psnr(update)=10.43\n",
      "[   9/ 510] loss(model)=0.0000337, loss(energy)=209402125.4400000, psnr=10.40, psnr(update)=10.40\n",
      "[  10/ 510] loss(model)=0.0000321, loss(energy)=210431024.6400000, psnr=10.48, psnr(update)=10.48\n",
      "[  11/ 510] loss(model)=0.0000318, loss(energy)=212082691.2000000, psnr=10.46, psnr(update)=10.46\n",
      "[  12/ 510] loss(model)=0.0000316, loss(energy)=211372171.5200000, psnr=10.46, psnr(update)=10.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_78474/3641450373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mscheduler_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mvalue_psnr\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mvalue_psnr_update\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_update\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_full_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_reduce_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36m_forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# calculate batch state and compute batch value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mbatch_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected all tensors to be on\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchmetrics/image/psnr.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# keep track of min and max target values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_squared_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "energy.train()\n",
    "sigma_np = np.logspace(0, np.log10(data_noise_sigma), optim_size_batch)\n",
    "sigma = torch.Tensor(sigma_np).view((optim_size_batch,1,1,1)).to(device)\n",
    "\n",
    "for i in range(optim_length_epoch):\n",
    "    val_loss_model  = []\n",
    "    val_loss_energy = []\n",
    "    val_psnr        = []\n",
    "    val_psnr_update = []\n",
    "\n",
    "    for j, (image, _) in enumerate(dataloader):\n",
    "        data, real  = torch.split(image, optim_size_batch, dim=0)\n",
    "        \n",
    "        noise       = torch.randn_like(data)\n",
    "        data_noise  = data + data_noise_sigma * noise\n",
    "        \n",
    "        noise       = torch.randn_like(data)\n",
    "        real_noise  = real + data_noise_sigma * noise\n",
    "     \n",
    "        data        = data.to(device) \n",
    "        real        = real.to(device) \n",
    "        data_noise  = data_noise.to(device)\n",
    "        real_noise  = real_noise.to(device)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # predictions\n",
    "        # -------------------------------------------------------------------         \n",
    "        pred, mu, log_var, z = model(data_noise)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # interpolation\n",
    "        # -------------------------------------------------------------------         \n",
    "        # alpha   = torch.rand(optim_size_batch, 1, 1, 1)\n",
    "        # alpha   = alpha.expand_as(real).to(device)\n",
    "        # interp  = alpha * real.data + (1 - alpha) * pred.data\n",
    "        # interp  = Parameter(interp, requires_grad=True)\n",
    "\n",
    "        pred = pred.requires_grad_()\n",
    "        # -------------------------------------------------------------------\n",
    "        # predictions\n",
    "        # -------------------------------------------------------------------\n",
    "        if model_use_dual_input:\n",
    "            energy_positive = energy(real_noise, real)        \n",
    "            energy_negative = energy(data_noise, pred)        \n",
    "            energy_interp   = energy(interp, interp)\n",
    "        else:\n",
    "            # energy_positive = energy(real)        \n",
    "            energy_negative = energy(pred)        \n",
    "            # energy_interp   = energy(interp)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # update energy model \n",
    "        # -------------------------------------------------------------------         \n",
    "        grad_x = torch.autograd.grad(energy_negative.sum(), pred, create_graph=True)[0]\n",
    "        optim_energy.zero_grad()\n",
    "        # loss_positive   = energy.compute_loss_positive(energy_negative, energy_positive)\n",
    "        # loss_gradient   = losses.compute_gradient_penalty(interp, energy_interp)\n",
    "        pred.detach()\n",
    "        loss_positive   = (((real - pred)/sigma/data_noise_sigma**2 + grad_x/sigma)**2).sum()\n",
    "        loss_energy     = loss_positive # + optim_weight_gradient * loss_gradient\n",
    "        loss_energy.backward()\n",
    "        optim_energy.step()\n",
    "        scheduler_energy.step(loss_energy)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # update input fake \n",
    "        # -------------------------------------------------------------------\n",
    "        pred_update = Parameter(pred, requires_grad=True) \n",
    "\n",
    "        energy.eval()\n",
    "        for k in range(optim_length_langevin): \n",
    "\n",
    "            if model_use_dual_input:\n",
    "                energy_negative = energy(data_noise, pred_update)\n",
    "            else:\n",
    "                energy_negative = energy(pred_update)\n",
    "            \n",
    "            loss_negative = energy.compute_loss_negative(energy_negative)\n",
    "            loss_negative.backward()\n",
    "            noise = torch.randn_like(pred_update.data)    # N(mean=0, std=1)\n",
    "            pred_update.data = pred_update.data - optim_lr_data * pred_update.grad + optim_lr_langevin * noise * data_noise_sigma\n",
    "            pred_update.grad.detach_()\n",
    "            pred_update.grad.zero_() \n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        # update model \n",
    "        # -------------------------------------------------------------------         \n",
    "        energy.train()\n",
    "        optim_model.zero_grad()\n",
    "        pred, mu, log_var, z = model(data_noise)\n",
    "        loss_model, loss_data, loss_regular = model.compute_loss(pred, pred_update, mu, log_var, optim_weight_regular) \n",
    "        loss_model.backward()\n",
    "        optim_model.step()        \n",
    "        scheduler_model.step(loss_model)\n",
    "\n",
    "        value_psnr          = psnr(pred.data, data).detach().cpu().numpy().mean()\n",
    "        value_psnr_update   = psnr(pred_update.data, data).detach().cpu().numpy().mean()\n",
    "            \n",
    "        val_loss_model.append(loss_model.item()) \n",
    "        val_loss_energy.append(loss_energy.item()) \n",
    "        val_psnr.append(value_psnr)\n",
    "        val_psnr_update.append(value_psnr_update)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        dir_log             = os.path.join(dir_work, 'log')\n",
    "        file_pred           = os.path.join(dir_log, 'image/pred.png')\n",
    "        file_pred_update    = os.path.join(dir_log, 'image/pred_update.png')\n",
    "        file_pred           = os.path.join(dir_log, 'image/{:03d}.png'.format(i))\n",
    "        file_pred_update    = os.path.join(dir_log, 'image/{:03d}_update.png'.format(i))\n",
    "        \n",
    "        save_image(pred.data[:25], file_pred, nrow=5, normalize=True)\n",
    "        save_image(pred_update.data[:25], file_pred_update, nrow=5, normalize=True)\n",
    "        \n",
    "    \n",
    "    val_loss_model_mean[i]  = np.mean(val_loss_model)\n",
    "    val_loss_model_std[i]   = np.std(val_loss_model)\n",
    "    val_loss_energy_mean[i] = np.mean(val_loss_energy)\n",
    "    val_loss_energy_std[i]  = np.std(val_loss_energy)\n",
    "    val_psnr_mean[i]        = np.mean(val_psnr)\n",
    "    val_psnr_std[i]         = np.std(val_psnr)\n",
    "    val_psnr_update_mean[i] = np.mean(val_psnr_update)\n",
    "    val_psnr_update_std[i]  = np.std(val_psnr_update)\n",
    "\n",
    "    log = '[%4d/%4d] loss(model)=%8.7f, loss(energy)=%8.7f, psnr=%4.2f, psnr(update)=%4.2f' % (i, optim_length_epoch, \n",
    "                        val_loss_model_mean[i], val_loss_energy_mean[i], val_psnr_mean[i], val_psnr_update_mean[i])\n",
    "    print(log, flush=True)\n",
    "    \n",
    "    if np.isnan(val_loss_model_mean[i]) or np.isnan(val_loss_energy_mean[i]) or val_psnr_mean[i] < 3:\n",
    "        sys.exit('error')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# save the models\n",
    "# -------------------------------------------------------------------          \n",
    "torch.save({\n",
    "    'state_dict_model'      : model.state_dict(),\n",
    "    'state_dict_energy'     : energy.state_dict(),\n",
    "    'model_conv'            : model_conv,\n",
    "    'model_activation'      : model_activation,\n",
    "    'model_output'          : model_output,\n",
    "    'model_use_batch_norm'  : model_use_batch_norm,\n",
    "    'model_use_skip'        : model_use_skip,\n",
    "    'model_use_dual_input'  : model_use_dual_input,\n",
    "    'model_dim_feature'     : model_dim_feature,\n",
    "    'model_dim_latent'      : model_dim_latent,\n",
    "    'data_name'             : data_name,\n",
    "    'data_use_all'          : data_use_all,\n",
    "    'data_label_subset'     : data_label_subset,\n",
    "    'data_channel'          : data_channel,\n",
    "    'data_size  '           : img_size,\n",
    "    'data_noise_sigma'      : data_noise_sigma,\n",
    "    'optim_option'          : optim_option,\n",
    "    'optim_length_epoch'    : optim_length_epoch,\n",
    "    'optim_size_batch'      : optim_size_batch,\n",
    "    'optim_lr_model'        : optim_lr_model,\n",
    "    'optim_lr_energy'       : optim_lr_energy,\n",
    "    'optim_lr_data'         : optim_lr_data,\n",
    "    'optim_lr_langevin'     : optim_lr_langevin,\n",
    "    'optim_length_langevin' : optim_length_langevin,\n",
    "    'optim_weight_gradient' : optim_weight_gradient,\n",
    "    'optim_weight_regular'  : optim_weight_regular,\n",
    "}, file_model)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# save the options\n",
    "# -------------------------------------------------------------------         \n",
    "with open(file_option, 'w') as f:\n",
    "    f.write('{}: {}\\n'.format('model_conv', model_conv))\n",
    "    f.write('{}: {}\\n'.format('model_activation', model_activation))\n",
    "    f.write('{}: {}\\n'.format('model_output', model_output))\n",
    "    f.write('{}: {}\\n'.format('model_use_batch_norm', model_use_batch_norm))\n",
    "    f.write('{}: {}\\n'.format('model_use_skip', model_use_skip))\n",
    "    f.write('{}: {}\\n'.format('model_use_dual_input', model_use_dual_input))\n",
    "    f.write('{}: {}\\n'.format('model_dim_feature', model_dim_feature))\n",
    "    f.write('{}: {}\\n'.format('model_dim_latent', model_dim_latent))\n",
    "    f.write('{}: {}\\n'.format('data_name', data_name))\n",
    "    f.write('{}: {}\\n'.format('data_use_all', data_use_all))\n",
    "    f.write('{}: {}\\n'.format('data_label_subset', data_label_subset))\n",
    "    f.write('{}: {}\\n'.format('data_channel', data_channel))\n",
    "    f.write('{}: {}\\n'.format('image_size', img_size))\n",
    "    f.write('{}: {}\\n'.format('data_noise_sigma', data_noise_sigma))\n",
    "    f.write('{}: {}\\n'.format('optim_option', optim_option))\n",
    "    f.write('{}: {}\\n'.format('optim_length_epoch', optim_length_epoch))\n",
    "    f.write('{}: {}\\n'.format('optim_size_batch', optim_size_batch))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_model', optim_lr_model))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_energy', optim_lr_energy))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_data', optim_lr_data))\n",
    "    f.write('{}: {}\\n'.format('optim_lr_langevin', optim_lr_langevin))\n",
    "    f.write('{}: {}\\n'.format('optim_length_langevin', optim_length_langevin))\n",
    "    f.write('{}: {}\\n'.format('optim_weight_gradient', optim_weight_gradient))\n",
    "    f.write('{}: {}\\n'.format('optim_weight_regular', optim_weight_regular))\n",
    "f.close()\n",
    "    \n",
    "# -------------------------------------------------------------------\n",
    "# save training results\n",
    "# -------------------------------------------------------------------         \n",
    "data        = data.detach().cpu().numpy().squeeze()\n",
    "data_noise  = data_noise.detach().cpu().numpy().squeeze()\n",
    "pred        = pred.detach().cpu().numpy().squeeze()\n",
    "pred_update = pred_update.detach().cpu().numpy().squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# save training results\n",
    "# -------------------------------------------------------------------         \n",
    "nRow    = 5 \n",
    "nCol    = 5\n",
    "fSize   = 3\n",
    "\n",
    "fig, ax = plt.subplots(nRow, nCol, figsize=(fSize * nCol, fSize * nRow))\n",
    "\n",
    "ax[0][0].set_title('loss (model)')\n",
    "ax[0][0].plot(val_loss_model_mean, color='red')\n",
    "ax[0][0].fill_between(list(range(optim_length_epoch)), val_loss_model_mean-val_loss_model_std, val_loss_model_mean+val_loss_model_std, color='blue', alpha=0.2)\n",
    "\n",
    "ax[0][1].set_title('loss (energy)')\n",
    "ax[0][1].plot(val_loss_energy_mean, color='red')\n",
    "ax[0][1].fill_between(list(range(optim_length_epoch)), val_loss_energy_mean-val_loss_energy_std, val_loss_energy_mean+val_loss_energy_std, color='blue', alpha=0.2)\n",
    "\n",
    "ax[0][2].set_title('psnr')\n",
    "ax[0][2].plot(val_psnr_mean, color='blue', label='y_0')\n",
    "ax[0][2].plot(val_psnr_update_mean, color='red', label='y_n')\n",
    "ax[0][2].legend()\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[1][i].set_title('clean')\n",
    "    ax[1][i].imshow(data[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[2][i].set_title('noisy')\n",
    "    ax[2][i].imshow(data_noise[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[3][i].set_title('y_0')\n",
    "    ax[3][i].imshow(pred[i], 'gray')\n",
    "\n",
    "for i in range(nCol):\n",
    "    ax[4][i].set_title('y_n')\n",
    "    ax[4][i].imshow(pred_update[i], 'gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(file_figure, bbox_inches='tight', dpi=300)\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3035e755443aca2cb3217176319455f49be3dbda138992f9ddc44b43d021598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
