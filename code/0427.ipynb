{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchmetrics import PeakSignalNoiseRatio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datetime, random\n",
    "import csv\n",
    "import os\n",
    "import argparse\n",
    "import models\n",
    "from utils import permute, to_numpy, init_weights, Self_Energy_log, get_dataloaders\n",
    "from utils import gaussian_noise, sp_noise, delete_square, generate_Y0\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"CIFAR10\")\n",
    "parser.add_argument(\"--data_size\", type=int, default=60000)\n",
    "parser.add_argument(\"--train_ratio\", type=float, default=1.0)\n",
    "parser.add_argument(\"--subset\", action=\"store_true\", default=True)\n",
    "parser.add_argument(\"--use_label\", type=int, default=5)\n",
    "parser.add_argument(\"--use_unpaired\", action=\"store_true\", default=False)\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--in_channel\", type=int, default=3)\n",
    "parser.add_argument(\"--img_size\", type=int, default=8)\n",
    "parser.add_argument(\"--dim_feature\", type=int, default=32)\n",
    "\n",
    "parser.add_argument(\"--gaussian_noise\", type=float, default=0.005)\n",
    "parser.add_argument(\"--sp_noise\", type=float, default=0.1)\n",
    "parser.add_argument(\"--square_pixels\", type=int, default=20)\n",
    "parser.add_argument(\"--degradation\", type=str, default='gaussian_noise')\n",
    "parser.add_argument(\"--Y0_type\", type=str, default='random')\n",
    "\n",
    "parser.add_argument(\"--lr_energy_model\", type=float, default=0.0001)\n",
    "parser.add_argument(\"--lr_langevin_min\", type=float, default=0.01)  \n",
    "parser.add_argument(\"--lr_langevin_max\", type=float, default=0.1)  \n",
    "parser.add_argument(\"--number_step_langevin\", type=int, default=50)  \n",
    "parser.add_argument(\"--use_energy_sched\", action=\"store_true\", default=False)  \n",
    "parser.add_argument(\"--regular_data\", type=float, default=0.0)\n",
    "parser.add_argument(\"--init_noise_decay\", type=float, default=1.0)\n",
    "# parser.add_argument(\"--use_gp\", action=\"store_true\", default=True)\n",
    "# parser.add_argument(\"--use_energy_reg\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--reg_w\", type=float, default=0.0005)\n",
    "# parser.add_argument(\"--use_energy_L2_reg\", action=\"store_true\", default=True)\n",
    "parser.add_argument(\"--L2_reg_w\", type=float, default=0.1)\n",
    "parser.add_argument('--num_workers', type=int, default=4, help='')\n",
    "\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--save_plot\", type=int, default=20)\n",
    "\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "transforms_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(args.img_size),\n",
    "    # torchvision.transforms.RandomCrop(32, padding=4),\n",
    "    # torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    # torchvision.transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "scale_range = [-1, 1]\n",
    "sched_step_size       = 1 # 10\n",
    "sched_gamma           = 0.93\n",
    "dim_output            = 1\n",
    "add_noise             = True\n",
    "dir_data = '/hdd1/dataset'\n",
    "list_lr_langevin      = np.linspace(args.lr_langevin_max, args.lr_langevin_min, num=args.epochs, endpoint=True)\n",
    "pl.seed_everything(0)\n",
    "\n",
    "train_loader, test_loader= get_dataloaders(dir_data, args.dataset, args.img_size, \n",
    "                                            args.batch_size, train_size = args.data_size,\n",
    "                                            transform=transforms_train, use_subset = args.subset,\n",
    "                                            parallel=False,\n",
    "                                            num_workers=args.num_workers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8039) tensor(0.0667) tensor(0.4184)\n"
     ]
    }
   ],
   "source": [
    "a, _ = next(iter(train_loader))\n",
    "print(torch.max(a), torch.min(a), torch.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0500]]],\n",
       "\n",
       "\n",
       "        [[[0.2000]]],\n",
       "\n",
       "\n",
       "        [[[0.3500]]],\n",
       "\n",
       "\n",
       "        [[[0.5000]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas_np = np.linspace(0.05, 0.5, args.batch_size)\n",
    "noise = torch.Tensor(sigmas_np).view((args.batch_size, 1, 1, 1))\n",
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
    "\n",
    "        # Series of convolutions and Swish activation functions\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(args.in_channel, args.dim_feature, 3, 2, padding=1), # [16x16] - Larger padding to get 32x32 image\n",
    "                Swish(),\n",
    "                nn.Conv2d(args.dim_feature, args.dim_feature * 2, 3,2,1), #  [8x8]\n",
    "                Swish(),\n",
    "                nn.Conv2d(args.dim_feature * 2, args.dim_feature * 4, kernel_size=3, stride=2, padding=1), # [4x4]\n",
    "                Swish(),\n",
    "                nn.Conv2d(args.dim_feature * 4,args.dim_feature*8, kernel_size=3, stride=2, padding=1), # [2x2]\n",
    "                Swish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(args.dim_feature*8 * 4, args.dim_feature*8),\n",
    "                Swish(),\n",
    "                nn.Linear(args.dim_feature*8, 1)\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                # nn.init.xavier_uniform_(m.weight.data)\n",
    "                #nn.init.uniform_(m.weight)\n",
    "                if m.bias is not None: \n",
    "                    nn.init.constant_(m.bias.data, 0)\n",
    "            \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "            \n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "\t\t\t\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                #nn.init.kaiming_uniform_(m.weight)\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                #nn.init.uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for getting a new batch of \"fake\" images.\n",
    "        Inputs:\n",
    "            steps - Number of iterations in the MCMC algorithm\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Choose 95% of the batch from the buffer, 5% generate from scratch \n",
    "        n_new = np.random.binomial(self.sample_size, 0.05)\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
    "        old_imgs = torch.cat(random.choices(self.examples, k=self.sample_size-n_new), dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
    "        \"\"\"\n",
    "        Function for sampling images for a given model. \n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
    "            steps - Number of iterations in the MCMC algorithm.\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
    "        \"\"\"\n",
    "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "        # because we are only interested in the gradients of the input. \n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "        \n",
    "        # Enable gradient calculation if not already the case\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "        # More efficient than creating a new tensor every iteration.\n",
    "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
    "        \n",
    "        # List for storing generations at each step (for later analysis)\n",
    "        imgs_per_step = []\n",
    "        \n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            # Part 1: Add noise to the input.\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            # Part 2: calculate gradients for the current input.\n",
    "            out_imgs = - model(inp_imgs)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "            # Apply gradients to our current samples\n",
    "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone().detach())\n",
    "        \n",
    "        # Reactivate gradients for parameters for training\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "        \n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SampleBuffer:\n",
    "    def __init__(self, max_samples=10000):\n",
    "        self.max_samples = max_samples\n",
    "        self.buffer = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, samples, class_ids=None):\n",
    "        samples = samples.detach().to('cpu')\n",
    "        class_ids = class_ids.detach().to('cpu')\n",
    "\n",
    "        for sample, class_id in zip(samples, class_ids):\n",
    "            self.buffer.append((sample.detach(), class_id))\n",
    "\n",
    "            if len(self.buffer) > self.max_samples:\n",
    "                self.buffer.pop(0)\n",
    "\n",
    "    def get(self, n_samples, device=device):\n",
    "        items = random.choices(self.buffer, k=n_samples)\n",
    "        samples, class_ids = zip(*items)\n",
    "        samples = torch.stack(samples, 0)\n",
    "        class_ids = torch.tensor(class_ids)\n",
    "        samples = samples.to(device)\n",
    "        class_ids = class_ids.to(device)\n",
    "\n",
    "        return samples, class_ids\n",
    "\n",
    "def sample_buffer(buffer, batch_size=args.batch_size, p=0.95, device=device):\n",
    "    if len(buffer) < 1:\n",
    "        return (\n",
    "            torch.rand(batch_size, 1, 32, 32, device=device),\n",
    "            torch.randint(0, 10, (batch_size,), device=device),\n",
    "        )\n",
    "\n",
    "    n_replay = (np.random.rand(batch_size) < p).sum()\n",
    "\n",
    "    replay_sample, replay_id = buffer.get(n_replay)\n",
    "    random_sample = torch.rand(batch_size - n_replay, 1, 32, 32, device=device)\n",
    "    random_id = torch.randint(0, 10, (batch_size - n_replay,), device=device)\n",
    "\n",
    "    return (\n",
    "        torch.cat([replay_sample, random_sample], 0),\n",
    "        torch.cat([replay_id, random_id], 0),\n",
    "    )\n",
    "                   \n",
    "def langevin(model, inp_imgs, epochs, lr_langevin, noise_decay=1.0, add_noise=True, return_img_per_step=False):\n",
    "    \"\"\"\n",
    "    Function for getting a new batch of \"fake\" images.\n",
    "    Inputs:\n",
    "        epochs, steps - Number of iterations in the MCMC algorithm\n",
    "        lr, step_size - Learning rate nu in the algorithm above\n",
    "    \"\"\"\n",
    "    # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "    # because we are only interested in the gradients of the input.\n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    inp_imgs.requires_grad = True\n",
    "\n",
    "    # Enable gradient calculation if not already the case\n",
    "    had_gradients_enabled = torch.is_grad_enabled()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "    # More efficient than creating a new tensor every iteration.\n",
    "    if add_noise:\n",
    "        noise_scale = np.sqrt(lr_langevin) * noise_decay\n",
    "        noise = 0.5 * torch.rand_like(inp_imgs) * noise_scale\n",
    "    else:\n",
    "        noise = 0.0\n",
    "\n",
    "# List for storing generations at each step (for later analysis)\n",
    "    imgs_per_step = []\n",
    "    \n",
    "    # Loop over K (steps)\n",
    "    for _ in range(epochs):\n",
    "        # Part 1: Add noise to the input.\n",
    "        noise.normal_(0, 0.005)\n",
    "        inp_imgs.data.add_(noise.data)\n",
    "        inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "        \n",
    "        # Part 2: calculate gradients for the current input.\n",
    "        out_imgs = -model(inp_imgs)\n",
    "        out_imgs.sum().backward()\n",
    "        inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "        # Apply gradients to our current samples\n",
    "        inp_imgs.data.add_(-lr_langevin * inp_imgs.grad.data)\n",
    "        inp_imgs.grad.detach_()\n",
    "        inp_imgs.grad.zero_()\n",
    "        inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "        \n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(inp_imgs.clone().detach())\n",
    "    \n",
    "    # Reactivate gradients for parameters for training\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    model.train(is_training)\n",
    "\n",
    "    # Reset gradient calculation to setting before this function\n",
    "    torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "    if return_img_per_step:\n",
    "        return torch.stack(imgs_per_step, dim=0)\n",
    "    else:\n",
    "        return inp_imgs\n",
    "    \n",
    "def update_langevin(model, inp, step, lr):\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    inp.requires_grad = True\n",
    "    noise = torch.randn(inp.shape, device=inp.device)\n",
    "    \n",
    "    for _ in range(step):\n",
    "        noise.normal_(0, 0.005)\n",
    "        inp.data.add_(noise.data)\n",
    "        inp.data.clamp_(min=-1.0, max=1.0)\n",
    "        \n",
    "        out = -model(inp)\n",
    "        out.sum().backward()\n",
    "        \n",
    "        inp.grad.data.add_(-0.5*lr*inp.grad.data)\n",
    "        inp.grad.detach_()\n",
    "        inp.grad.zero_()\n",
    "        inp.data.clamp_(min=-1.0, max=1.0)\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    model.train()\n",
    "    \n",
    "    return inp    \n",
    "    \n",
    "    \n",
    "def sample_langevin(x, model, stepsize, n_steps, noise_scale=None, intermediate_samples=False):\n",
    "    \"\"\"Draw samples using Langevin dynamics\n",
    "    x: torch.Tensor, initial points\n",
    "    model: An energy-based model\n",
    "    stepsize: float\n",
    "    n_steps: integer\n",
    "    noise_scale: Optional. float. If None, set to np.sqrt(stepsize * 2)\n",
    "    \"\"\"\n",
    "    if noise_scale is None:\n",
    "        noise_scale = np.sqrt(stepsize * 2)\n",
    "\n",
    "    l_samples = []\n",
    "    l_dynamics = []\n",
    "    x.requires_grad = True\n",
    "    for _ in range(n_steps):\n",
    "        l_samples.append(x.detach().to('cpu'))\n",
    "        noise = torch.randn_like(x) * noise_scale\n",
    "        out = model(x)\n",
    "        grad = torch.autograd.grad(out.sum(), x, only_inputs=True)[0]\n",
    "        dynamics = stepsize * grad + noise\n",
    "        x = x + dynamics\n",
    "        l_samples.append(x.detach().to('cpu'))\n",
    "        l_dynamics.append(dynamics.detach().to('cpu'))\n",
    "\n",
    "    if intermediate_samples:\n",
    "        return l_samples, l_dynamics\n",
    "    else:\n",
    "        return l_samples[-1]\n",
    "    \n",
    "def clip_grad(parameters, optimizer):\n",
    "    with torch.no_grad():\n",
    "        for group in optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = optimizer.state[p]\n",
    "\n",
    "                if 'step' not in state or state['step'] < 1:\n",
    "                    continue\n",
    "\n",
    "                step = state['step']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                _, beta2 = group['betas']\n",
    "\n",
    "                bound = 3 * torch.sqrt(exp_avg_sq / (1 - beta2 ** step)) + 0.1\n",
    "                p.grad.data.copy_(torch.max(torch.min(p.grad.data, bound), -bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9248, grad_fn=<SumBackward0>)\n",
      "tensor(0.0221) tensor(0.0221)\n"
     ]
    }
   ],
   "source": [
    "en = CNNModel(args)\n",
    "image, _ = next(iter(dl_train))\n",
    "z = torch.randn(image.shape) # randn_like(image) 와 동일\n",
    "z.requires_grad = True\n",
    "# print(torch.max(z), torch.min(z), torch.mean(z)) # tensor(4.4663) tensor(-4.3208) tensor(-0.0002)\n",
    "out = en(z)\n",
    "print(out.sum())\n",
    "out.sum().backward()\n",
    "print(z.grad.data.sum(), z.grad.sum(()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 16, 16]             320\n",
      "             Swish-2           [-1, 32, 16, 16]               0\n",
      "            Conv2d-3             [-1, 64, 8, 8]          18,496\n",
      "             Swish-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5            [-1, 128, 4, 4]          73,856\n",
      "             Swish-6            [-1, 128, 4, 4]               0\n",
      "            Conv2d-7            [-1, 256, 2, 2]         295,168\n",
      "             Swish-8            [-1, 256, 2, 2]               0\n",
      "           Flatten-9                 [-1, 1024]               0\n",
      "           Linear-10                  [-1, 256]         262,400\n",
      "            Swish-11                  [-1, 256]               0\n",
      "           Linear-12                    [-1, 1]             257\n",
      "================================================================\n",
      "Total params: 650,497\n",
      "Trainable params: 650,497\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.25\n",
      "Params size (MB): 2.48\n",
      "Estimated Total Size (MB): 2.73\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "en = CNNModel(args).to(device)\n",
    "\n",
    "print(torchsummary.summary(en, (1,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = CNNModel(args).to(device)\n",
    "\n",
    "optimizer = optim.Adam(energy.parameters(), lr=args.lr_energy_model, betas=(args.b1, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=sched_step_size, gamma=sched_gamma) # Exponential decay over epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd1/anaconda/minhyeok/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0/ 100] loss=28.658, loss_contrastive_divergence=12.633, loss_regularization=16.025, psnr=  nan, lan_psnr=3.470, metrics_avg_real=0.017, metrics_avg_fake=12.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hdd1/anaconda/minhyeok/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/hdd1/anaconda/minhyeok/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/ 100] loss=29.076, loss_contrastive_divergence=12.751, loss_regularization=16.324, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.769\n",
      "[   2/ 100] loss=29.263, loss_contrastive_divergence=12.804, loss_regularization=16.458, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.017, metrics_avg_fake=12.822\n",
      "[   3/ 100] loss=29.418, loss_contrastive_divergence=12.848, loss_regularization=16.570, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.018, metrics_avg_fake=12.865\n",
      "[   4/ 100] loss=29.544, loss_contrastive_divergence=12.883, loss_regularization=16.661, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.017, metrics_avg_fake=12.900\n",
      "[   5/ 100] loss=29.525, loss_contrastive_divergence=12.878, loss_regularization=16.648, psnr=  nan, lan_psnr=3.458, metrics_avg_real=0.017, metrics_avg_fake=12.895\n",
      "[   6/ 100] loss=29.588, loss_contrastive_divergence=12.895, loss_regularization=16.693, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.018, metrics_avg_fake=12.913\n",
      "[   7/ 100] loss=29.641, loss_contrastive_divergence=12.910, loss_regularization=16.731, psnr=  nan, lan_psnr=3.458, metrics_avg_real=0.018, metrics_avg_fake=12.928\n",
      "[   8/ 100] loss=29.621, loss_contrastive_divergence=12.904, loss_regularization=16.717, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.018, metrics_avg_fake=12.922\n",
      "[   9/ 100] loss=29.688, loss_contrastive_divergence=12.923, loss_regularization=16.765, psnr=  nan, lan_psnr=3.456, metrics_avg_real=0.017, metrics_avg_fake=12.941\n",
      "[  10/ 100] loss=29.651, loss_contrastive_divergence=12.913, loss_regularization=16.738, psnr=  nan, lan_psnr=3.455, metrics_avg_real=0.017, metrics_avg_fake=12.930\n",
      "[  11/ 100] loss=29.604, loss_contrastive_divergence=12.899, loss_regularization=16.705, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.018, metrics_avg_fake=12.917\n",
      "[  12/ 100] loss=29.650, loss_contrastive_divergence=12.912, loss_regularization=16.738, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.018, metrics_avg_fake=12.930\n",
      "[  13/ 100] loss=29.668, loss_contrastive_divergence=12.917, loss_regularization=16.751, psnr=  nan, lan_psnr=3.455, metrics_avg_real=0.018, metrics_avg_fake=12.935\n",
      "[  14/ 100] loss=29.670, loss_contrastive_divergence=12.917, loss_regularization=16.752, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.018, metrics_avg_fake=12.935\n",
      "[  15/ 100] loss=29.661, loss_contrastive_divergence=12.915, loss_regularization=16.746, psnr=  nan, lan_psnr=3.456, metrics_avg_real=0.018, metrics_avg_fake=12.933\n",
      "[  16/ 100] loss=29.648, loss_contrastive_divergence=12.912, loss_regularization=16.737, psnr=  nan, lan_psnr=3.460, metrics_avg_real=0.018, metrics_avg_fake=12.929\n",
      "[  17/ 100] loss=29.632, loss_contrastive_divergence=12.907, loss_regularization=16.725, psnr=  nan, lan_psnr=3.456, metrics_avg_real=0.017, metrics_avg_fake=12.924\n",
      "[  18/ 100] loss=29.635, loss_contrastive_divergence=12.908, loss_regularization=16.727, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.017, metrics_avg_fake=12.925\n",
      "[  19/ 100] loss=29.674, loss_contrastive_divergence=12.919, loss_regularization=16.755, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.017, metrics_avg_fake=12.936\n",
      "[  20/ 100] loss=29.660, loss_contrastive_divergence=12.915, loss_regularization=16.745, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.932\n",
      "[  21/ 100] loss=29.686, loss_contrastive_divergence=12.922, loss_regularization=16.764, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.940\n",
      "[  22/ 100] loss=29.661, loss_contrastive_divergence=12.915, loss_regularization=16.746, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.933\n",
      "[  23/ 100] loss=29.599, loss_contrastive_divergence=12.898, loss_regularization=16.701, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.915\n",
      "[  24/ 100] loss=29.620, loss_contrastive_divergence=12.903, loss_regularization=16.716, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.921\n",
      "[  25/ 100] loss=29.614, loss_contrastive_divergence=12.902, loss_regularization=16.712, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.919\n",
      "[  26/ 100] loss=29.582, loss_contrastive_divergence=12.893, loss_regularization=16.689, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.910\n",
      "[  27/ 100] loss=29.591, loss_contrastive_divergence=12.896, loss_regularization=16.696, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.913\n",
      "[  28/ 100] loss=29.577, loss_contrastive_divergence=12.892, loss_regularization=16.685, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.909\n",
      "[  29/ 100] loss=29.642, loss_contrastive_divergence=12.910, loss_regularization=16.732, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.017, metrics_avg_fake=12.927\n",
      "[  30/ 100] loss=29.647, loss_contrastive_divergence=12.911, loss_regularization=16.736, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.929\n",
      "[  31/ 100] loss=29.672, loss_contrastive_divergence=12.919, loss_regularization=16.754, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.017, metrics_avg_fake=12.936\n",
      "[  32/ 100] loss=29.684, loss_contrastive_divergence=12.922, loss_regularization=16.762, psnr=  nan, lan_psnr=3.460, metrics_avg_real=0.017, metrics_avg_fake=12.939\n",
      "[  33/ 100] loss=29.686, loss_contrastive_divergence=12.922, loss_regularization=16.764, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.018, metrics_avg_fake=12.940\n",
      "[  34/ 100] loss=29.673, loss_contrastive_divergence=12.918, loss_regularization=16.754, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.018, metrics_avg_fake=12.936\n",
      "[  35/ 100] loss=29.662, loss_contrastive_divergence=12.915, loss_regularization=16.747, psnr=  nan, lan_psnr=3.458, metrics_avg_real=0.018, metrics_avg_fake=12.933\n",
      "[  36/ 100] loss=29.674, loss_contrastive_divergence=12.919, loss_regularization=16.756, psnr=  nan, lan_psnr=3.457, metrics_avg_real=0.017, metrics_avg_fake=12.936\n",
      "[  37/ 100] loss=29.716, loss_contrastive_divergence=12.930, loss_regularization=16.786, psnr=  nan, lan_psnr=3.460, metrics_avg_real=0.018, metrics_avg_fake=12.948\n",
      "[  38/ 100] loss=29.692, loss_contrastive_divergence=12.923, loss_regularization=16.768, psnr=  nan, lan_psnr=3.460, metrics_avg_real=0.018, metrics_avg_fake=12.941\n",
      "[  39/ 100] loss=29.702, loss_contrastive_divergence=12.926, loss_regularization=16.776, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.017, metrics_avg_fake=12.944\n",
      "[  40/ 100] loss=29.664, loss_contrastive_divergence=12.916, loss_regularization=16.749, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.933\n",
      "[  41/ 100] loss=29.655, loss_contrastive_divergence=12.913, loss_regularization=16.742, psnr=  nan, lan_psnr=3.465, metrics_avg_real=0.018, metrics_avg_fake=12.931\n",
      "[  42/ 100] loss=29.598, loss_contrastive_divergence=12.897, loss_regularization=16.701, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.915\n",
      "[  43/ 100] loss=29.595, loss_contrastive_divergence=12.896, loss_regularization=16.698, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.914\n",
      "[  44/ 100] loss=29.595, loss_contrastive_divergence=12.896, loss_regularization=16.698, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.018, metrics_avg_fake=12.914\n",
      "[  45/ 100] loss=29.609, loss_contrastive_divergence=12.900, loss_regularization=16.708, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.918\n",
      "[  46/ 100] loss=29.627, loss_contrastive_divergence=12.905, loss_regularization=16.722, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.923\n",
      "[  47/ 100] loss=29.595, loss_contrastive_divergence=12.897, loss_regularization=16.698, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.914\n",
      "[  48/ 100] loss=29.632, loss_contrastive_divergence=12.907, loss_regularization=16.725, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.924\n",
      "[  49/ 100] loss=29.535, loss_contrastive_divergence=12.880, loss_regularization=16.655, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.897\n",
      "[  50/ 100] loss=29.577, loss_contrastive_divergence=12.892, loss_regularization=16.685, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.017, metrics_avg_fake=12.909\n",
      "[  51/ 100] loss=29.640, loss_contrastive_divergence=12.909, loss_regularization=16.731, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.018, metrics_avg_fake=12.927\n",
      "[  52/ 100] loss=29.603, loss_contrastive_divergence=12.899, loss_regularization=16.704, psnr=  nan, lan_psnr=3.460, metrics_avg_real=0.018, metrics_avg_fake=12.917\n",
      "[  53/ 100] loss=29.625, loss_contrastive_divergence=12.905, loss_regularization=16.719, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.923\n",
      "[  54/ 100] loss=29.622, loss_contrastive_divergence=12.904, loss_regularization=16.717, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.922\n",
      "[  55/ 100] loss=29.658, loss_contrastive_divergence=12.915, loss_regularization=16.744, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.932\n",
      "[  56/ 100] loss=29.630, loss_contrastive_divergence=12.907, loss_regularization=16.723, psnr=  nan, lan_psnr=3.465, metrics_avg_real=0.018, metrics_avg_fake=12.924\n",
      "[  57/ 100] loss=29.657, loss_contrastive_divergence=12.914, loss_regularization=16.743, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.932\n",
      "[  58/ 100] loss=29.651, loss_contrastive_divergence=12.913, loss_regularization=16.738, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.017, metrics_avg_fake=12.930\n",
      "[  59/ 100] loss=29.668, loss_contrastive_divergence=12.918, loss_regularization=16.751, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.935\n",
      "[  60/ 100] loss=29.600, loss_contrastive_divergence=12.898, loss_regularization=16.701, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.017, metrics_avg_fake=12.916\n",
      "[  61/ 100] loss=29.642, loss_contrastive_divergence=12.910, loss_regularization=16.732, psnr=  nan, lan_psnr=3.466, metrics_avg_real=0.018, metrics_avg_fake=12.928\n",
      "[  62/ 100] loss=29.649, loss_contrastive_divergence=12.912, loss_regularization=16.737, psnr=  nan, lan_psnr=3.466, metrics_avg_real=0.017, metrics_avg_fake=12.930\n",
      "[  63/ 100] loss=29.678, loss_contrastive_divergence=12.920, loss_regularization=16.758, psnr=  nan, lan_psnr=3.463, metrics_avg_real=0.018, metrics_avg_fake=12.938\n",
      "[  64/ 100] loss=29.681, loss_contrastive_divergence=12.921, loss_regularization=16.760, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.017, metrics_avg_fake=12.938\n",
      "[  65/ 100] loss=29.680, loss_contrastive_divergence=12.921, loss_regularization=16.760, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.017, metrics_avg_fake=12.938\n",
      "[  66/ 100] loss=29.715, loss_contrastive_divergence=12.930, loss_regularization=16.785, psnr=  nan, lan_psnr=3.459, metrics_avg_real=0.018, metrics_avg_fake=12.948\n",
      "[  67/ 100] loss=29.720, loss_contrastive_divergence=12.932, loss_regularization=16.788, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.018, metrics_avg_fake=12.949\n",
      "[  68/ 100] loss=29.741, loss_contrastive_divergence=12.938, loss_regularization=16.804, psnr=  nan, lan_psnr=3.461, metrics_avg_real=0.017, metrics_avg_fake=12.955\n",
      "[  69/ 100] loss=29.726, loss_contrastive_divergence=12.934, loss_regularization=16.793, psnr=  nan, lan_psnr=3.462, metrics_avg_real=0.017, metrics_avg_fake=12.951\n",
      "[  70/ 100] loss=29.746, loss_contrastive_divergence=12.939, loss_regularization=16.807, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.017, metrics_avg_fake=12.956\n",
      "[  71/ 100] loss=29.728, loss_contrastive_divergence=12.934, loss_regularization=16.794, psnr=  nan, lan_psnr=3.464, metrics_avg_real=0.017, metrics_avg_fake=12.951\n",
      "[  72/ 100] loss=29.772, loss_contrastive_divergence=12.946, loss_regularization=16.826, psnr=  nan, lan_psnr=3.465, metrics_avg_real=0.017, metrics_avg_fake=12.963\n",
      "[  73/ 100] loss=29.786, loss_contrastive_divergence=12.950, loss_regularization=16.837, psnr=  nan, lan_psnr=3.465, metrics_avg_real=0.018, metrics_avg_fake=12.967\n",
      "[  74/ 100] loss=29.770, loss_contrastive_divergence=12.945, loss_regularization=16.825, psnr=  nan, lan_psnr=3.467, metrics_avg_real=0.017, metrics_avg_fake=12.962\n",
      "[  75/ 100] loss=29.751, loss_contrastive_divergence=12.940, loss_regularization=16.811, psnr=  nan, lan_psnr=3.467, metrics_avg_real=0.018, metrics_avg_fake=12.957\n",
      "[  76/ 100] loss=29.742, loss_contrastive_divergence=12.937, loss_regularization=16.805, psnr=  nan, lan_psnr=3.468, metrics_avg_real=0.017, metrics_avg_fake=12.955\n",
      "[  77/ 100] loss=29.673, loss_contrastive_divergence=12.918, loss_regularization=16.754, psnr=  nan, lan_psnr=3.467, metrics_avg_real=0.017, metrics_avg_fake=12.936\n",
      "[  78/ 100] loss=29.641, loss_contrastive_divergence=12.909, loss_regularization=16.732, psnr=  nan, lan_psnr=3.466, metrics_avg_real=0.018, metrics_avg_fake=12.927\n",
      "[  79/ 100] loss=29.646, loss_contrastive_divergence=12.911, loss_regularization=16.735, psnr=  nan, lan_psnr=3.466, metrics_avg_real=0.018, metrics_avg_fake=12.928\n",
      "[  80/ 100] loss=29.693, loss_contrastive_divergence=12.924, loss_regularization=16.769, psnr=  nan, lan_psnr=3.469, metrics_avg_real=0.018, metrics_avg_fake=12.941\n",
      "[  81/ 100] loss=29.694, loss_contrastive_divergence=12.924, loss_regularization=16.770, psnr=  nan, lan_psnr=3.470, metrics_avg_real=0.018, metrics_avg_fake=12.942\n",
      "[  82/ 100] loss=29.701, loss_contrastive_divergence=12.926, loss_regularization=16.775, psnr=  nan, lan_psnr=3.471, metrics_avg_real=0.018, metrics_avg_fake=12.944\n",
      "[  83/ 100] loss=29.740, loss_contrastive_divergence=12.937, loss_regularization=16.803, psnr=  nan, lan_psnr=3.470, metrics_avg_real=0.017, metrics_avg_fake=12.955\n",
      "[  84/ 100] loss=29.720, loss_contrastive_divergence=12.932, loss_regularization=16.789, psnr=  nan, lan_psnr=3.470, metrics_avg_real=0.018, metrics_avg_fake=12.949\n",
      "[  85/ 100] loss=29.724, loss_contrastive_divergence=12.933, loss_regularization=16.791, psnr=  nan, lan_psnr=3.472, metrics_avg_real=0.018, metrics_avg_fake=12.950\n",
      "[  86/ 100] loss=29.708, loss_contrastive_divergence=12.928, loss_regularization=16.780, psnr=  nan, lan_psnr=3.472, metrics_avg_real=0.017, metrics_avg_fake=12.946\n",
      "[  87/ 100] loss=29.716, loss_contrastive_divergence=12.931, loss_regularization=16.785, psnr=  nan, lan_psnr=3.468, metrics_avg_real=0.018, metrics_avg_fake=12.948\n",
      "[  88/ 100] loss=29.724, loss_contrastive_divergence=12.933, loss_regularization=16.791, psnr=  nan, lan_psnr=3.469, metrics_avg_real=0.018, metrics_avg_fake=12.950\n",
      "[  89/ 100] loss=29.714, loss_contrastive_divergence=12.930, loss_regularization=16.784, psnr=  nan, lan_psnr=3.469, metrics_avg_real=0.018, metrics_avg_fake=12.948\n",
      "[  90/ 100] loss=29.698, loss_contrastive_divergence=12.925, loss_regularization=16.772, psnr=  nan, lan_psnr=3.468, metrics_avg_real=0.018, metrics_avg_fake=12.943\n",
      "[  91/ 100] loss=29.768, loss_contrastive_divergence=12.945, loss_regularization=16.822, psnr=  nan, lan_psnr=3.471, metrics_avg_real=0.017, metrics_avg_fake=12.963\n",
      "[  92/ 100] loss=29.738, loss_contrastive_divergence=12.937, loss_regularization=16.801, psnr=  nan, lan_psnr=3.466, metrics_avg_real=0.018, metrics_avg_fake=12.954\n",
      "[  93/ 100] loss=29.766, loss_contrastive_divergence=12.945, loss_regularization=16.821, psnr=  nan, lan_psnr=3.468, metrics_avg_real=0.017, metrics_avg_fake=12.962\n",
      "[  94/ 100] loss=29.653, loss_contrastive_divergence=12.913, loss_regularization=16.740, psnr=  nan, lan_psnr=3.469, metrics_avg_real=0.018, metrics_avg_fake=12.931\n",
      "[  95/ 100] loss=29.662, loss_contrastive_divergence=12.916, loss_regularization=16.746, psnr=  nan, lan_psnr=3.468, metrics_avg_real=0.018, metrics_avg_fake=12.933\n",
      "[  96/ 100] loss=29.617, loss_contrastive_divergence=12.903, loss_regularization=16.714, psnr=  nan, lan_psnr=3.467, metrics_avg_real=0.018, metrics_avg_fake=12.920\n",
      "[  97/ 100] loss=29.605, loss_contrastive_divergence=12.899, loss_regularization=16.705, psnr=  nan, lan_psnr=3.469, metrics_avg_real=0.018, metrics_avg_fake=12.917\n",
      "[  98/ 100] loss=29.647, loss_contrastive_divergence=12.911, loss_regularization=16.736, psnr=  nan, lan_psnr=3.469, metrics_avg_real=0.017, metrics_avg_fake=12.929\n",
      "[  99/ 100] loss=29.658, loss_contrastive_divergence=12.915, loss_regularization=16.743, psnr=  nan, lan_psnr=3.465, metrics_avg_real=0.018, metrics_avg_fake=12.932\n"
     ]
    }
   ],
   "source": [
    "# noise = torch.randn(args.batch_size, 1, 32, 32) * args.gaussian_noise\n",
    "# noise = noise.to(device)\n",
    "# buffer = SampleBuffer()\n",
    "sampler = Sampler(energy, (1,32,32), args.batch_size)\n",
    "\n",
    "val_loss_energy_model_mean = np.zeros(args.epochs)\n",
    "val_psnr_mean              = np.zeros(args.epochs)\n",
    "val_psnr_langevin_mean     = np.zeros(args.epochs)\n",
    "\n",
    "\n",
    "def to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
    "  return tensor.detach().cpu().numpy()\n",
    "\n",
    "for i in range(args.epochs):\n",
    "  val_loss_energy_model = list()\n",
    "  val_loss_cd = list()\n",
    "  val_loss_reg = list()\n",
    "  val_psnr              = list()\n",
    "  val_psnr_langevin     = list()\n",
    "  val_fake = list()\n",
    "  val_real = list()\n",
    "\n",
    "  for j, (image, _) in enumerate(iter(dl_train)):\n",
    "\n",
    "    noise = torch.randn_like(image) * args.gaussian_noise\n",
    "    image.add_(noise).clamp_(min=-1.0, max=1.0)\n",
    "    real_imgs = image.to(device)\n",
    "    \n",
    "    fake_imgs = sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "    # Predict energy score for all images\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "    real_out, fake_out = energy(inp_imgs).chunk(2, dim=0)\n",
    "    \n",
    "    # Calculate losses\n",
    "    reg_loss = args.L2_reg_w * (real_out ** 2 + fake_out ** 2).mean()\n",
    "    cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "    loss = reg_loss + cdiv_loss\n",
    "    loss.backward()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # val_ = to_numpy(PSNR(fake_out, image))\n",
    "    val_lan = to_numpy(PSNR(fake_imgs, real_imgs))\n",
    "    \n",
    "    # Logging\n",
    "    val_loss_energy_model.append(loss.item())\n",
    "    val_loss_cd.append(cdiv_loss.item())\n",
    "    val_loss_reg.append(reg_loss.item())\n",
    "    # val_psnr.append(val_)\n",
    "    val_psnr_langevin.append(val_lan)\n",
    "    val_real.append(real_out.detach().to(torch.device('cpu')).mean())\n",
    "    val_fake.append(fake_out.detach().to(torch.device('cpu')).mean())\n",
    "    \n",
    "  log = '[%4d/%4d] loss=%5.3f, loss_contrastive_divergence=%5.3f, loss_regularization=%5.3f, psnr=%5.3f, lan_psnr=%5.3f, metrics_avg_real=%5.3f, metrics_avg_fake=%5.3f' % (i, args.epochs, \n",
    "                       np.mean(val_loss_energy_model), np.mean(val_loss_cd), np.mean(val_loss_reg), \n",
    "                       np.mean(val_psnr), np.mean(val_psnr_langevin), \n",
    "                       np.mean(val_real), np.mean(val_fake))\n",
    "  print(log, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    neg_img, _ = sample_buffer(buffer, image.shape[0]) # random variables\n",
    "    neg_img.requires_grad = True\n",
    "    # fake_imgs = langevin(energy, latent, 10, list_lr_langevin[i])\n",
    "    for p in energy.parameters():\n",
    "        p.requires_grad = False\n",
    "    energy.eval()\n",
    "    \n",
    "    for _ in range(args.number_step_langevin):\n",
    "        if noise.shape[0] != neg_img.shape[0]:\n",
    "            noise = torch.randn(image.shape[0], 1, 32, 32, device=device)\n",
    "                \n",
    "        noise.normal_(0, 0.005)\n",
    "        neg_img.data.add_(noise.data)\n",
    "        \n",
    "        neg_out = -energy(neg_img)\n",
    "        neg_out.sum().backward()\n",
    "        neg_img.grad.data.clamp_(-0.03, 0.03)\n",
    "\n",
    "        neg_img.data.add_(-list_lr_langevin[i] * neg_img.grad.data)\n",
    "\n",
    "        neg_img.grad.detach_()\n",
    "        neg_img.grad.zero_()\n",
    "\n",
    "        neg_img.data.clamp_(0, 1)\n",
    "        \n",
    "    neg_img = neg_img.detach()\n",
    "    for p in energy.parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    energy.train()\n",
    "    # fake_imgs = update_langevin(energy, latent, args.number_step_langevin, list_lr_langevin[i])\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pos_out = energy(image)\n",
    "    neg_out = energy(neg_img)\n",
    "\n",
    "    loss = args.energy_L2_reg_weight * (pos_out ** 2 + neg_out ** 2)\n",
    "    loss = loss + ( -pos_out + neg_out)\n",
    "    loss = loss.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    clip_grad(energy.parameters(), optimizer)\n",
    "    \n",
    "    value_psnr = to_numpy(PSNR(image, neg_img))\n",
    "    scheduler.step()\n",
    "    \n",
    "    val_loss_energy_model.append(loss.item())\n",
    "    val_psnr.append(value_psnr)\n",
    "    \n",
    "  loss = np.mean(val_loss_energy_model)\n",
    "  val_psnr_mean = np.mean(val_psnr)\n",
    "    \n",
    "\n",
    "  log = '[%4d/%4d] loss=%5.3f, psnr=%5.3f' % (i, args.epochs, loss, val_psnr_mean)\n",
    "  print(log, flush=True)\n",
    "  \n",
    "  if i % args.save_plot == 19:\n",
    "    nRow    = 4 \n",
    "    nCol    = 4\n",
    "    fSize   = 3\n",
    "\n",
    "    fig, ax = plt.subplots(nRow, nCol, figsize=(fSize * nCol, fSize * nRow))\n",
    "\n",
    "    for r in range(2): \n",
    "        for c in range(nCol):\n",
    "            ax[r+0][c].set_title('data')\n",
    "            if args.in_channel == 1: \n",
    "                p = ax[r+0][c].imshow(image[r*nCol+c].cpu().numpy().squeeze(), cmap='gray')\n",
    "            else:\n",
    "                p = ax[r+0][c].imshow(image[r*nCol+c].cpu().numpy().permute(1,2,0))\n",
    "            plt.colorbar(p, ax=ax[r+0][c])\n",
    "    \n",
    "    for r in range(2):    \n",
    "        for c in range(nCol):\n",
    "            ax[r+2][c].set_title('fake')\n",
    "            if args.in_channel == 1: \n",
    "                p = ax[r+2][c].imshow(neg_img[r*nCol+c].detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "            else:\n",
    "                p = ax[r+2][c].imshow(neg_img[r*nCol+c].detach().cpu().numpy().permute(1,2,0))\n",
    "            plt.colorbar(p, ax=ax[r+2][c])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args.epochs):\n",
    "  val_loss_energy_model = list()\n",
    "  val_psnr              = list()\n",
    "  val_psnr_langevin     = list()\n",
    "\n",
    "  for j, (image, _) in enumerate(iter(dl_train)):\n",
    "\n",
    "    image = image.to(device)\n",
    "    noise = torch.randn_like(image) * args.gaussian_noise\n",
    "    noisy_image = image.add_(noise).clamp(min=-1.0, max=1.0).to(device)\n",
    "    \n",
    "    latent = torch.randn_like(image.clone()).to(device)\n",
    "    \n",
    "    fake_imgs = langevin(energy, latent, 10, list_lr_langevin[i])\n",
    "    # fake_imgs = update_langevin(energy, latent, args.number_step_langevin, list_lr_langevin[i])\n",
    "    fake_imgs = fake_imgs.to(device)\n",
    "    input_imgs = torch.cat([noisy_image, fake_imgs], dim=0)\n",
    "    \n",
    "    real, fake = energy(input_imgs).chunk(2, dim=0)\n",
    "    \n",
    "    energy.zero_grad()\n",
    "    reg_loss = args.energy_L2_reg_weight * (real**2 + fake**2).sum()\n",
    "    cd_loss = fake.sum() - real.sum()\n",
    "    loss = cd_loss + reg_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    value_psnr = to_numpy(PSNR(image, fake_imgs))\n",
    "    scheduler.step()\n",
    "    \n",
    "    val_loss_energy_model.append(loss.item())\n",
    "    val_psnr.append(value_psnr)\n",
    "    \n",
    "  loss = np.mean(val_loss_energy_model)\n",
    "  val_psnr_mean = np.mean(val_psnr)\n",
    "    \n",
    "\n",
    "  log = '[%4d/%4d] loss=%5.3f, psnr=%5.3f' % (i, args.epochs, loss, val_psnr_mean)\n",
    "  print(log, flush=True)\n",
    "  \n",
    "  if i % args.save_plot == 19:\n",
    "    nRow    = 4 \n",
    "    nCol    = 4\n",
    "    fSize   = 3\n",
    "\n",
    "    fig, ax = plt.subplots(nRow, nCol, figsize=(fSize * nCol, fSize * nRow))\n",
    "\n",
    "    for r in range(2): \n",
    "        for c in range(nCol):\n",
    "            ax[r+0][c].set_title('data')\n",
    "            if args.in_channel == 1: \n",
    "                p = ax[r+0][c].imshow(image[r*nCol+c].cpu().numpy().squeeze(), cmap='gray')\n",
    "            else:\n",
    "                p = ax[r+0][c].imshow(image[r*nCol+c].cpu().numpy().permute(1,2,0))\n",
    "            plt.colorbar(p, ax=ax[r+0][c])\n",
    "    \n",
    "    for r in range(2):    \n",
    "        for c in range(nCol):\n",
    "            ax[r+2][c].set_title('fake')\n",
    "            if args.in_channel == 1: \n",
    "                p = ax[r+2][c].imshow(fake_imgs[r*nCol+c].detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "            else:\n",
    "                p = ax[r+2][c].imshow(fake_imgs[r*nCol+c].detach().cpu().numpy().permute(1,2,0))\n",
    "            plt.colorbar(p, ax=ax[r+2][c])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = np.random.binomial(args.batch_size, 0.05)\n",
    "rand_imgs = torch.rand((n_new,) + (1,32,32)) * 2 - 1\n",
    "old_imgs = torch.cat(random.choices(examples, k=args.batch_size-n_new), dim=0)\n",
    "inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 32, 32])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_imgs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
